<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="../www.googletagmanager.com/gtag/js-id=UA-111713571-1.js" tppabs="https://www.googletagmanager.com/gtag/js?id=UA-111713571-1"></script>
	<script>
  		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
  		gtag('js', new Date());
  		gtag('config', 'UA-111713571-1');
	</script>

	<!-- Show more content -->
	<script type="text/javascript">
		function toggle_vis(id) {
	    // var e = document.getElementById(id);
	    var e = document.getElementsByClassName(id);
			var showText = document.getElementById("showText");
			for (var i = 0; i < e.length; i++) {
		    	if (e[i].style.display == "none") {
		        	e[i].style.display = "inline";
	    			showText.innerHTML = "[Show less]";
		    	} else {
		    		e[i].style.display = "none";
		    		showText.innerHTML = "[Show more]";
		    	}
		    }
	    }
	</script>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="../maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" tppabs="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="../use.fontawesome.com/releases/v5.3.1/css/all.css" tppabs="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="../cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css" tppabs="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
  <link rel="stylesheet" href="static/styles.css" tppabs="https://yunzhuli.github.io/static/styles.css">
	<!-- Custom styles for this template -->
	<link href="files/jumbotron.css" tppabs="https://yunzhuli.github.io/files/jumbotron.css" rel="stylesheet">
	<script src="js/main.js" tppabs="https://yunzhuli.github.io/js/main.js"></script>
  <script src="js/scroll.js" tppabs="https://yunzhuli.github.io/js/scroll.js"></script>
</head>

<title>Yunzhu Li</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
	    <div class="container">
		<a class="navbar-brand" href="#Home">Yunzhu Li</a>

		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="#Home">Home</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Group">Group</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Talks">Talks</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Publications">Publications</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#Honors">Honors</a>
				</li>
			</ul>
		</div>
	    </div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 17px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 40px">
				<br>
				<img class="img-responsive img-rounded" src="files/profile_yunzhu_0.jpeg" tppabs="https://yunzhuli.github.io/files/profile_yunzhu_0.jpeg" alt="Photo" style="max-width: 100%; border:1px solid black"><br>
			</div>

			<div class="col-md-9">
			<br>
			<p><font style="font-size: 20px"><b>Yunzhu Li</b></font></p>

			<p>I am an Assistant Professor of <a href="javascript:if(confirm(%27https://cs.illinois.edu/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://cs.illinois.edu/%27" tppabs="https://cs.illinois.edu/" target="_blank">Computer Science</a> at the <a href="javascript:if(confirm(%27https://illinois.edu/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://illinois.edu/%27" tppabs="https://illinois.edu/" target="_blank">University of Illinois Urbana-Champaign (UIUC)</a> with affiliations in both <a href="javascript:if(confirm(%27https://ece.illinois.edu/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://ece.illinois.edu/%27" tppabs="https://ece.illinois.edu/" target="_blank">Electrical & Computer Engineering</a> and the <a href="javascript:if(confirm(%27https://csl.illinois.edu/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://csl.illinois.edu/%27" tppabs="https://csl.illinois.edu/" target="_blank">Coordinated Science Laboratory</a>.
			</p>

			<p>Before joining UIUC, I was a  Postdoc at <a href="javascript:if(confirm(%27https://svl.stanford.edu/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://svl.stanford.edu/%27" tppabs="https://svl.stanford.edu/" target="_blank">Stanford Vision and Learning Lab (SVL)</a>, working with <a href="javascript:if(confirm(%27https://profiles.stanford.edu/fei-fei-li  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://profiles.stanford.edu/fei-fei-li%27" tppabs="https://profiles.stanford.edu/fei-fei-li" target='_blank'>Fei-Fei Li</a> and <a href="javascript:if(confirm(%27https://jiajunwu.com/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://jiajunwu.com/%27" tppabs="https://jiajunwu.com/" target="_blank">Jiajun Wu</a>. I received my PhD from <a href="javascript:if(confirm(%27http://www.csail.mit.edu/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27http://www.csail.mit.edu/%27" tppabs="http://www.csail.mit.edu/" target="_blank">Computer Science and Artificial Intelligence Laboratory (CSAIL)</a> at  <a href="javascript:if(confirm(%27http://www.mit.edu/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27http://www.mit.edu/%27" tppabs="http://www.mit.edu/" target="_blank">MIT</a>, advised by <a href="javascript:if(confirm(%27http://web.mit.edu/torralba/www/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27http://web.mit.edu/torralba/www/%27" tppabs="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a> and <a href="javascript:if(confirm(%27https://groups.csail.mit.edu/locomotion/russt.html  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://groups.csail.mit.edu/locomotion/russt.html%27" tppabs="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and obtained my bachelor's degree from <a href="javascript:if(confirm(%27http://english.pku.edu.cn/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27http://english.pku.edu.cn/%27" tppabs="http://english.pku.edu.cn/" target="_blank">Peking University</a>.
			</p>

			<p>
			<a target="_blank" href="javascript:if(confirm(%27https://scholar.google.com/citations?user=WlA92lcAAAAJ&hl=en  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://scholar.google.com/citations?user=WlA92lcAAAAJ&hl=en%27" tppabs="https://scholar.google.com/citations?user=WlA92lcAAAAJ&hl=en"><font color="black"><i class="ai ai-google-scholar ai-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			<a target="_blank" href="javascript:if(confirm(%27https://github.com/yunzhuli  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://github.com/yunzhuli%27" tppabs="https://github.com/yunzhuli"><font color="black"><i class="fab fa-github fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			<a target="_blank" href="javascript:if(confirm(%27https://twitter.com/YunzhuLiYZ  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://twitter.com/YunzhuLiYZ%27" tppabs="https://twitter.com/YunzhuLiYZ"><font color="black"><i class="fab fa-twitter fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
			yunzhuli [at] illinois (dot) edu
			</p>

			</div>
		</div>
	</div><br><br>


  <!-- News -->
	<div class="container">
		<h3 id="Group" style="padding-top: 80px; margin-top: -80px;">Robotic Perception, Interaction, and Learning Lab (RoboPIL)</h3>
		<ul>
			<li>
				At <b><font color="firebrick">RoboPIL</font></b>, we work at the intersection of robotics, computer vision, and machine learning. Specifically, we focus on <b><font color="firebrick">Robot Learning</font></b> and aim to significantly expand robots' perception and physical interaction capabilities, particularly through the following three directions.
				<ul>
					<li><font color="firebrick"><b>Intuitive Physics:</b></font> Learning structured world models for robotic manipulation of objects with diverse physical properties.</li>
					<li><font color="firebrick"><b>Embodied Intelligence:</b></font> Focusing on long-horizon planning, generalization to diverse environments, and sim-to-real transfer.</li>
					<li><font color="firebrick"><b>Multi-Modal Perception:</b></font> Harnessing vision, touch, audio, and language for fine-grained and effective manipulation.
				</ul>
			</li>
			<li>
				<b>Prospective Students:</b> I'm looking for technically strong and self-motivated students passionate about advancing the frontiers of robot learning.
				<ul>
					<li><b>PhD Applicants:</b> Please submit your <a href="javascript:if(confirm(%27https://grad.illinois.edu/admissions/apply  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://grad.illinois.edu/admissions/apply%27" tppabs="https://grad.illinois.edu/admissions/apply" target="_blank">application</a> to the CS PhD program, and mention me as one of your Faculty of Interest.</li>
					<li><b>Postdoc and Visiting Students:</b> Please reach out to me directly via email, attaching your CV.
				</ul>
			</li>

			
			<br>

			<li>
				<b>PhD Students</b>
				<br><br>
				<div class="row">
					<div class="col-md-2" style="padding-right: 0px">
					    <div class="container">
					    	<div class="row">
					    		<div class="col-md-10" style="padding-right: 0px">
					                <img class="img-responsive img-rounded" src="files/students/BinghaoHuang.jpeg" tppabs="https://yunzhuli.github.io/files/students/BinghaoHuang.jpeg" alt="Photo" style="max-width: 100%; border:1px solid black">
					                <br>
					                <a href="javascript:if(confirm(%27https://binghao-huang.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://binghao-huang.github.io/%27" tppabs="https://binghao-huang.github.io/" target="_blank">Binghao Huang</a>
					                <br>
					                CS PhD
				            	</div>
			            	</div>
					    </div>
					</div>

					<div class="col-md-2" style="padding-right: 0px">
					    <div class="container">
					    	<div class="row">
					    		<div class="col-md-10" style="padding-right: 0px">
					    			<img class="img-responsive img-rounded" src="files/students/HanxiaoJiang.jpg" tppabs="https://yunzhuli.github.io/files/students/HanxiaoJiang.jpg" alt="Photo" style="max-width: 100%; border:1px solid black"><br>
									<a href="javascript:if(confirm(%27https://jianghanxiao.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://jianghanxiao.github.io/%27" tppabs="https://jianghanxiao.github.io/" target="_blank">Hanxiao Jiang</a>
									<br>
									CS PhD, with Shenlong Wang
								</div>
			            	</div>
					    </div>
					</div>

					<div class="col-md-2" style="padding-right: 0px">
					    <div class="container">
					    	<div class="row">
					    		<div class="col-md-10" style="padding-right: 0px">
					                <img class="img-responsive img-rounded" src="files/students/ShivanshPatel.jpeg" tppabs="https://yunzhuli.github.io/files/students/ShivanshPatel.jpeg" alt="Photo" style="max-width: 100%; border:1px solid black">
					                <br>
					                <a href="javascript:if(confirm(%27https://shivanshpatel35.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://shivanshpatel35.github.io/%27" tppabs="https://shivanshpatel35.github.io/" target="_blank">Shivansh Patel</a>
					                <br>
					                CS PhD
				            	</div>
			            	</div>
					    </div>
					</div>

					<div class="col-md-2" style="padding-right: 0px">
					    <div class="container">
					    	<div class="row">
					    		<div class="col-md-10" style="padding-right: 0px">
					    			<img class="img-responsive img-rounded" src="files/students/YixuanWang.jpg" tppabs="https://yunzhuli.github.io/files/students/YixuanWang.jpg" alt="Photo" style="max-width: 100%; border:1px solid black">
					    			<br>
									<a href="javascript:if(confirm(%27https://wangyixuan12.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://wangyixuan12.github.io/%27" tppabs="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>
									<br>
									ECE PhD
				            	</div>
			            	</div>
					    </div>
					</div>

					<div class="col-md-2" style="padding-right: 0px">
					    <div class="container">
					    	<div class="row">
					    		<div class="col-md-10" style="padding-right: 0px">
					                <img class="img-responsive img-rounded" src="files/students/RuihaiWu.jpeg" tppabs="https://yunzhuli.github.io/files/students/RuihaiWu.jpeg" alt="Photo" style="max-width: 100%; border:1px solid black">
					                <br>
									<a href="javascript:if(confirm(%27https://warshallrho.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://warshallrho.github.io/%27" tppabs="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a>
					                <br>
					                Visiting PhD
				            	</div>
			            	</div>
					    </div>
					</div>

					<div class="col-md-2" style="padding-right: 0px">
					    <div class="container">
					    	<div class="row">
					    		<div class="col-md-10" style="padding-right: 0px">
					                <img class="img-responsive img-rounded" src="files/students/KaifengZhang.jpg" tppabs="https://yunzhuli.github.io/files/students/KaifengZhang.jpg" alt="Photo" style="max-width: 100%; border:1px solid black">
									<a href="javascript:if(confirm(%27https://kywind.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://kywind.github.io/%27" tppabs="https://kywind.github.io/" target="_blank">Kaifeng Zhang</a>
									<br>
									CS PhD
									<br>
				            	</div>
			            	</div>
					    </div>
					</div>

				</div>
			</li>

			<br>

			<li>
				<b>Master Students</b>
				<div class="row">
					<div class="col-md-6", style="padding-right: 0px">
						<ul>
							<li><a href="javascript:if(confirm(%27https://www.linkedin.com/in/baoyu-li-b1646b220/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.linkedin.com/in/baoyu-li-b1646b220/%27" tppabs="https://www.linkedin.com/in/baoyu-li-b1646b220/" target="_blank">Baoyu Li</a> (CS Master, with Kris Hauser)</li>
							<li><a href="javascript:if(confirm(%27https://ece.illinois.edu/about/directory/grad-students/keyis2  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://ece.illinois.edu/about/directory/grad-students/keyis2%27" tppabs="https://ece.illinois.edu/about/directory/grad-students/keyis2" target="_blank">Keyi Shen</a> (ECE Master, with Huan Zhang)</li>
						</ul>
					</div>
					<div class="col-md-6", style="padding-right: 0px">
						<ul>
							<li><a href="" target="_blank">Mingtong Zhang</a> (CS Master)</li>
						</ul>
					</div>
				</div>
			</li>

			<br>

			<!--
			<li>
				<b>Former Mentees</b>
				<div class="row">
					<div class="col-md-6", style="padding-right: 0px">
						<ul>
							<li><a href="https://caiyancheng.github.io/academic.html" target="_blank">Yancheng Cai</a> <small>(was Visiting Student at Stanford, now PhD at Cambridge)</small></li>
							<li><a href="https://dou-yiming.github.io/" target="_blank">Yiming Dou</a> <small>(was Visiting Student at Stanford, now PhD at UMich)</small></li>
							<li><a href="https://haolirobo.github.io/" target="_blank">Hao Li</a> <small>(was Master at Stanford, now PhD at Stanford)</small></li>
							<li><a href="https://toruowo.github.io/" target="_blank">Toru Lin</a> <small>(was Undergrad at MIT, now PhD at UC Berkeley)</small></li>
						</ul>
					</div>
					<div class="col-md-6", style="padding-right: 0px">
						<ul>
							<li><a href="https://www.ziangliu.com/" target="_blank">Ziang Liu</a> <small>(was Master at Stanford, now PhD at Cornell)</small></li>
							<li><a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a> <small>(was Master at Stanford, now PhD at Stanford)</small></li>
							<li><a href="https://xavihart.github.io/" target="_blank">Haotian Xue</a> <small>(was Visiting Student at MIT, now PhD at Georgia Tech)</small></li>
							<li><a href="https://scholar.google.com/citations?user=mapNJjcAAAAJ" target="_blank">Qiang Zhang</a> <small>(was Visiting Student at MIT, now PhD at Princeton)</small></li>
						</ul>
					</div>

				</div>
			</li>
			-->

		</ul>
	</div><br><br>


	<!-- Talks -->
	<div class="container">
		<h3 id="Talks" style="padding-top: 80px; margin-top: -80px;">
			Recent Talks
		</h3>
		<hr>

		<h5>Learning Structured World Models From and For Physical Interactions</h5>

		<div class="row">
			<div class="col-md-4">
				[2023/06] <b>CVPR-23 Precognition Workshop</b>
				<iframe src="../www.youtube.com/embed/5BvDfA68sJE" tppabs="https://www.youtube.com/embed/5BvDfA68sJE" width="360" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</div>
			<div class="col-md-4">
				[2022/06] <b>PhD Thesis Defense at MIT CSAIL</b>
				<iframe src="../www.youtube.com/embed/8eof6F4-r0k" tppabs="https://www.youtube.com/embed/8eof6F4-r0k" width="360" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</div>
		</div>
		<hr>

		<ul>
			<li>[2024/05] Invited Talk at <b>ICRA 2024 Workshop on <a href="javascript:if(confirm(%27https://icra-manipulation-skill.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://icra-manipulation-skill.github.io/%27" tppabs="https://icra-manipulation-skill.github.io/" target="_blank">A Future Roadmap for Sensorimotor Skill Learning for Robot Manipulation</a></b></li>
			<li>[2024/05] Invited Talk at <b>ICRA 2024 Workshop on <a href="javascript:if(confirm(%27https://sites.google.com/view/icra2024-ws-bi-mp/home  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://sites.google.com/view/icra2024-ws-bi-mp/home%27" tppabs="https://sites.google.com/view/icra2024-ws-bi-mp/home" target="_blank">Bimanual Manipulation: On Kitchen Challenges</a></b></li>
			<li>[2024/03] Invited Talk at <b>CMU Robotics Institute</b></li>
			<li>[2024/03] Guest Lecture for <b><a href="javascript:if(confirm(%27https://16-831-s24.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://16-831-s24.github.io/%27" tppabs="https://16-831-s24.github.io/" target="_blank">16-831: Introduction to Robot Learning</a></b> at <b>CMU</b></li>
			<li>[2024/03] Invited Talk at <b><a href="javascript:if(confirm(%27https://aice.illinois.edu/events  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://aice.illinois.edu/events%27" tppabs="https://aice.illinois.edu/events" target="_blank">AICE Spring Research Symposium</a></b></li>
			<li>[2024/01] Invited Talk at <b><a href="javascript:if(confirm(%27https://twitter.com/MontrealRobots/status/1750312111094346042  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://twitter.com/MontrealRobots/status/1750312111094346042%27" tppabs="https://twitter.com/MontrealRobots/status/1750312111094346042" target="_blank">Mila Robot Learning Seminar</a></b> <a href="javascript:if(confirm(%27https://www.youtube.com/watch?v=qTNuXY5-k_M  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.youtube.com/watch?v=qTNuXY5-k_M%27" tppabs="https://www.youtube.com/watch?v=qTNuXY5-k_M" target="_blank">[Recording]</a></li>
			<li>[2023/12] Invited Talk at <b>Amazon Science</b></li>
			<li>[2023/11] Invited Talk at <b>Tsinghua University, <a href="javascript:if(confirm(%27https://iiis.tsinghua.edu.cn/en/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://iiis.tsinghua.edu.cn/en/%27" tppabs="https://iiis.tsinghua.edu.cn/en/" target="_blank">Institute for Interdisciplinary Information Sciences (IIIS)</a></b></li>
			<li>[2023/10] Invited Talk at <b>IROS 2023 Workshop on <a href="javascript:if(confirm(%27https://sites.google.com/view/iros23-causal-robots/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://sites.google.com/view/iros23-causal-robots/%27" tppabs="https://sites.google.com/view/iros23-causal-robots/" target="_blank">Causality for Robotics</a></b></li>
			<li>[2023/09] Invited Talk at <b><a href="javascript:if(confirm(%27https://robotics.illinois.edu/robotics-seminar-series/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://robotics.illinois.edu/robotics-seminar-series/%27" tppabs="https://robotics.illinois.edu/robotics-seminar-series/" target="_blank">Robotics Seminar @ Illinois</a></b>
			<li>[2023/08] Invited Talk at <b>Stanford Intelligent Systems Laboratory</b></li>
			<li>[2023/07] Invited Talk at <b>Peking University</b></li>
			<li>[2023/06] Invited Talk at <b>CVPR 2023 Workshop on <a href="javascript:if(confirm(%27https://sites.google.com/view/ieeecvf-cvpr2023-precognition  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://sites.google.com/view/ieeecvf-cvpr2023-precognition%27" tppabs="https://sites.google.com/view/ieeecvf-cvpr2023-precognition" target="_blank">Precognition: Seeing through the Future</a></b> <a href="javascript:if(confirm(%27https://www.youtube.com/watch?v=5BvDfA68sJE  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.youtube.com/watch?v=5BvDfA68sJE%27" tppabs="https://www.youtube.com/watch?v=5BvDfA68sJE" target="_blank">[Recording]</a></li>
			<li>[2023/05] Invited Talk at <b>ICRA 2023 Workshop on <a href="javascript:if(confirm(%27https://deformable-workshop.github.io/icra2023/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://deformable-workshop.github.io/icra2023/%27" tppabs="https://deformable-workshop.github.io/icra2023/" target="_blank">Representing and Manipulating Deformable Objects</a></b></li>
			<div class="talks" style="display:none">
				<li>[2022/12] Guest Lecture at <b>NYU</b>, Invited Talk at <b>Shanghai AI Laboratory</b></li>
				<li>[2022/10] Invited Talk at <b>Stanford</b> and <b>UC Berkeley</b></li>
				<li>[2022/07] Invited Talk at <b>RSS 2022 Workshop on <a href="javascript:if(confirm(%27https://imrss2022.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://imrss2022.github.io/%27" tppabs="https://imrss2022.github.io/" target="_blank">Implicit Representations for Robotic Manipulation</a></b> <a href="javascript:if(confirm(%27https://www.youtube.com/watch?v=gawSKjA9skU  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.youtube.com/watch?v=gawSKjA9skU%27" tppabs="https://www.youtube.com/watch?v=gawSKjA9skU" target="_blank">[Recording]</a> and <b>ISEE AI</b></li>
				<li>[2022/06] <b>PhD Thesis Defense at MIT CSAIL</b> <a href="javascript:if(confirm(%27https://www.youtube.com/watch?v=8eof6F4-r0k  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.youtube.com/watch?v=8eof6F4-r0k%27" tppabs="https://www.youtube.com/watch?v=8eof6F4-r0k" target="_blank">[Recording]</a>
				<li>[2022/04] Invited Talk at <b>Google Research</b></li>
				<li>[2022/01] Invited Talk at <b>TU Berlin</b></li>
				<li>[2021/12] Invited Talk at <b>Meta AI Research</b></li>
				<li>[2021/11] Invited Talk at <b>Toyota Research Institute</b></li>
				<li>[2021/08] Invited Talk at <b>Wayve.ai</b> and <b>Tsinghua University</b></li>
				<li>[2021/06] Invited Talk at <b>CVPR 2021 Tutorial on <a href="javascript:if(confirm(%27https://xiaolonw.github.io/graphnnv3/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://xiaolonw.github.io/graphnnv3/%27" tppabs="https://xiaolonw.github.io/graphnnv3/" target="_blank">Learning Representations via Graph-structured Networks</a></b> <a href="javascript:if(confirm(%27https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s%27" tppabs="https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s" target="_blank">[Recording]</a></li>
				<li>[2021/05] Invited Talk at <b>ICLR 2021 Workshop on <a href="javascript:if(confirm(%27https://simdl.github.io/overview/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://simdl.github.io/overview/%27" tppabs="https://simdl.github.io/overview/" target="_blank">Deep Learning for Simulation</a></b> <a href="javascript:if(confirm(%27https://youtu.be/sfblDZeyjk8  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://youtu.be/sfblDZeyjk8%27" tppabs="https://youtu.be/sfblDZeyjk8" target="_blank">[Recording]</a></li>
				<li>[2021/04] Invited Talk at <b>Brown University</b></li>
				<li>[2021/03] Invited Talk at <b>Extrality.ai</b></li>
				<li>[2020/10] Invited Talk at <b>Stanford</b> and <b>UMass Amherst</b></li>
				<li>[2020/09] Invited Talk at the <b>University of Toronto</b> <a href="javascript:if(confirm(%27https://www.youtube.com/watch?v=y_j53bkKzq8  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.youtube.com/watch?v=y_j53bkKzq8%27" tppabs="https://www.youtube.com/watch?v=y_j53bkKzq8" target="_blank">[Recording]</a></li>
			</div>
			<li style="font-size: 16px">
				<a href="javascript:toggle_vis('talks')" id="showText">[Show more]</a>
			</li>
		</ul>

		<!--
		<div class="row">
			<div class="col-md-2">
				<a href="https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s" target="_blank"><img class="img-fluid img-rounded" src="talks/20210620_CVPR21_GraphTutorial/logo.jpg" style="border:1px solid black" alt=""></a>
			</div>

			<div class="col-md-10">
				Graph-Structured Networks for Physical Inference and Model-Based Control
				<br>
				Invited talk at <b><font color="black">CVPR 2021 Tutorial <a href="https://xiaolonw.github.io/graphnnv3/" target="_blank">Learning Representations via Graph-structured Networks</a></font></b>
				<br>
				<a href="https://www.youtube.com/watch?v=ost49rbnI_8&t=7207s" target="_blank">[YouTube]</a> (June 2021)
			</div>
		</div><hr>

		<div class="row">
			<div class="col-md-2">
				<a href="https://youtu.be/sfblDZeyjk8" target="_blank"><img class="img-fluid img-rounded" src="talks/20210507_ICLR21_simDL/logo.jpg" style="border:1px solid black" alt=""></a>
			</div>

			<div class="col-md-10">
				Learning Compositional Dynamics Models for Physical Inference and Model-Based Control
				<br>
				Invited talk at <b><font color="black">ICLR 2021 Workshop <a href="https://simdl.github.io/overview/" target="_blank">Deep Learning for Simulation</a></font></b>
				<br>
				<a href="https://youtu.be/sfblDZeyjk8" target="_blank">[YouTube]</a> (May 2021)
			</div>
		</div><hr>
		--!>
	</div><br><br>



	<!-- Publications -->
	<div class="container">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
			Publications
			<small><small>
			(<a href="" id="select0" onclick="showPubs(0); return false;">show selected</a> /
			 <a href="" id="select1" onclick="showPubs(1); return false;">show by date</a> /
       			 <a href="" id="select2" onclick="showPubs(2); return false;">show by topic</a>)
			</small></small><br>

			<small><small>
		  	<font color="black">Research Topics:
				<a href="#manip" onclick="showPubs(2)">Robotic Manipulation</a> /
				<a href="#phys" onclick="showPubs(2)">Physical Scene Understanding</a> /
				<a href="#multi" onclick="showPubs(2)">Multi-Modal Perception</a> /
				<a href="#imi" onclick="showPubs(2)">Imitation Learning</a>
			</font><br>
			</small></small>
		</h3>

		<div id="pubs"></div>

		<script id="pubs_selected" language="text">
			<font color="black">(* indicates equal contribution)</font><br><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 1px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="https://yunzhuli.github.io/projects/robocook/spotlight_robocook.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://samuelpclarke.com/" target="_blank">Samuel Clarke</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://hshi74.github.io/robocook/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2306.14447" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/robocook" target="_blank"> <small>[Code]</small></a>
				<a href="https://hshi74.github.io/robocook/bibtex.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Best Systems Paper Award</b></font>
				</div>
			</div><hr>

			<div class="row">
			  <div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 1px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="https://yunzhuli.github.io/projects/VoxPoser/spotlight_voxposer.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank">Ruohan Zhang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>, and
				<a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a>
				<br>
				<b><font color="black">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://voxposer.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2307.05973" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=Yvn4eR05A3M" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/VoxPoser/VoxPoser.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stow/stow.jpg"/*tpa=https://yunzhuli.github.io/projects/stow/stow.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=SomOgNIAAAAJ&hl=en" target="_blank">Haonan Chen</a>,
				<a href="https://www.linkedin.com/in/yilong-niu-a13b5b289/" target="_blank">Yilong Niu</a>*,
				<a href="https://www.linkedin.com/in/kaiwen-hong-524520141/?locale=en_US" target="_blank">Kaiwen Hong</a>*,
				<a href="https://shuijing725.github.io/" target="_blank">Shuijing Liu</a>,
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>
				<br>
				<b><font color="black">Predicting Object Interactions with Behavior Primitives: An Application in Stowing Tasks</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://haonan16.github.io/stow_page/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2309.16873" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/haonan16/Stow/" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/stow/stow.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Finalist - Best Paper/Best Student Paper Awards</b></font>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dyn-res/dyn-res.gif"/*tpa=https://yunzhuli.github.io/projects/dyn-res/dyn-res.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Dynamic-Resolution Model Learning for Object Pile Manipulation</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2023</a></b>,
				<a href="https://robopil.github.io/dyn-res-pile-manip/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.16700" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/WangYixuan12/dyn-res-pile-manip" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/dyn-res/dyn-res.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Best Paper Award</b></font> at <b>IROS 2023</b> Workshop on Learning Meets Model-based Methods <a href="https://sites.google.com/view/learning-meets-models-iros2023" target="_blank"><small>[Link]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/sparse-dynamics/sparse-dynamics.gif"/*tpa=https://yunzhuli.github.io/projects/sparse-dynamics/sparse-dynamics.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.ziangliu.com/" target="_blank">Ziang Liu</a>,
				<a href="https://www.linkedin.com/in/g9zhou" target="_blank">Genggeng Zhou</a>*,
				<a href="https://openreview.net/profile?id=~Jeff_He1" target="_blank">Jeff He</a>*,
				<a href="https://tobiamarcucci.github.io/" target="_blank">Tobia Marcucci</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a>, and
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>
				<br>
				<b><font color="black">Model-Based Control with Sparse Neural Dynamics</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b>,
				<a href="https://robopil.github.io/Sparse-Dynamics/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2312.12791" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/sparse-dynamics/sparse-dynamics.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-dy/nerf-dy-multiview.gif"/*tpa=https://yunzhuli.github.io/projects/nerf-dy/nerf-dy-multiview.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="https://people.csail.mit.edu/lishuang/" target="_blank">Shuang Li</a>*,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>,
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a></b>,
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="https://youtu.be/ELPMiifELGc" target="_blank"> <small>[Video]</small></a>
				<a href="https://openreview.net/forum?id=zv3NYgRZ7Qo" target="_blank"> <small>[OpenReview]</small></a>
				<a href="https://3d-representation-learning.github.io/nerf-dy/nerf-dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font><br>
				Abridged in <b>RSS 2021</b> Workshop on Visual Learning and Reasoning for Robotics <a href="https://rssvlrr.github.io/" target="_blank"><small>[Link]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/senstextile/senstextile.jpg"/*tpa=https://yunzhuli.github.io/projects/senstextile/senstextile.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="http://people.csail.mit.edu/kuiwu" target="_blank">Kui Wu</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/beichen-li" target="_blank">Beichen Li</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning Human-environment Interactions using Conformal Tactile Textiles</font></b><br>
				<b><a href="https://www.nature.com/natelectron/" target="_blank">Nature Electronics</a></b> 4, 193–201 (2021),
				<font color="firebrick"><b>5-year Impact Factor: 33.695</b></font>
				<br>
				<a href="http://senstextile.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41928-021-00558-0" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/senstextile" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/senstextile/senstextile.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Featured on the</small>
				<a href="https://www.nature.com/natelectron/volumes/4/issues/3" target="_blank"> <small>cover</small></a>
				<small>of the issue.</small>
				<small>Editorial comments</small>
				<a href="https://www.nature.com/articles/s41928-021-00567-z" target="_blank"> <small>[Link]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://www.nature.com/articles/s41928-021-00560-6" target="_blank"> <small>[Nature Electronics News & Views]</small></a>
				<a href="https://www.csail.mit.edu/news/smart-clothes-can-measure-your-movements" target="_blank"> <small>[MIT CSAIL News]</small></a>
				<a href="https://gizmodo.com/researchers-might-have-finally-cracked-smart-clothing-1846546202" target="_blank"> <small>[Gizmodo]</small></a>
				<a href="https://www.engadget.com/mit-csail-smart-clothes-track-movements-160010512.html" target="_blank"> <small>[Engadget]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/v-cdn/v-cdn.gif"/*tpa=https://yunzhuli.github.io/projects/v-cdn/v-cdn.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Animashree Anandkumar</a>,
				<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, and
				<a href="https://animesh.garg.tech/" target="_blank">Animesh Garg</a>
				<br>
				<b><font color="black">Causal Discovery in Physical Systems from Videos</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://yunzhuli.github.io/www/V-CDN/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2007.00631" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/pairlab/v-cdn" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=hRsCt8xLn_8" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/poster.pdf" target="_blank"> <small>[Poster]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/V-CDN.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://venturebeat.com/2020/07/02/ai-system-learns-to-model-how-fabrics-interact-by-watching-videos/" target="_blank"> <small>[VentureBeat]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/compkpm/compkpm.gif"/*tpa=https://yunzhuli.github.io/projects/compkpm/compkpm.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>*,
				<a href="http://people.csail.mit.edu/hehaodele/" target="_blank">Hao He</a>*,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/dina/" target="_blank">Dina Katabi</a>, and
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Compositional Koopman Operators for Model-Based Control</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://koopman.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=H1ldzA4tPr" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/CompositionalKoopmanOperators" target="_blank"><small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/compkpm/compkpm.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://youtu.be/MnXo_hjh1Q4" target="_blank"> <small>[Video]</small></a>
				<a href="http://koopman.csail.mit.edu/poster.pdf" target="_blank"> <small>[Poster]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation</b></font><br>
				Abridged in <b>NeurIPS 2019</b> Workshop on Graph Representation Learning <a href="https://grlearning.github.io/papers/" target="_blank"><small>[Link]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stag/stag_lowres.jpg"/*tpa=https://yunzhuli.github.io/projects/stag/stag_lowres.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/subras/" target="_blank">Subramanian Sundaram</a>,
				<a href="https://people.csail.mit.edu/pkellnho/" target="_blank">Petr Kellnhofer</a>,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://people.csail.mit.edu/wojciech/" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning the Signatures of the Human Grasp Using a Scalable Tactile Glove</font></b><br>
				<b><a href="https://www.nature.com/" target="_blank">Nature</a></b> 569, 698–702 (2019),
				<font color="firebrick"><b>5-year Impact Factor: 54.637</b></font>
				<br>
				<a href="http://stag.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41586-019-1234-z" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/Erkil1452/touch" target="_blank"> <small>[Code]</small></a>
				<a href="http://stag.csail.mit.edu/files/sundaram2019stag.bib" target="_blank"> <small>[BibTex]</small></a>,

				<small>Collected by</small>
				<a href="projects/stag/stag_mit_museum.jpg"/*tpa=https://yunzhuli.github.io/projects/stag/stag_mit_museum.jpg*/ target="_blank"> <small>MIT Museum</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.nature.com/articles/d41586-019-01593-w" target="_blank"> <small>[Nature News & Views]</small></a>
				<a href="https://devicematerialscommunity.nature.com/users/257334-subramanian-sundaram/posts/49420-learning-dexterity-from-humans" target="_blank"> <small>[Nature communities]</small></a>
				<a href="https://www.economist.com/science-and-technology/2019/05/30/improving-robots-grasp-requires-a-new-way-to-measure-it-in-humans" target="_blank"> <small>[The Economist]</small></a>
				<a href="https://www.pbs.org/wgbh/nova/article/electronic-glove-pressure-sensors/" target="_blank"> <small>[PBS NOVA]</small></a>
				<a href="https://www.bbc.co.uk/sounds/play/p079yr9y" target="_blank"> <small>[BBC Radio]</small></a>
				<a href="https://www.newscientist.com/article/2204736-smart-glove-works-out-what-youre-holding-from-its-weight-and-shape/" target="_blank"> <small>[NewScientist]</small></a>
				</div>
			</div><hr>

      			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dpi/dpi.png"/*tpa=https://yunzhuli.github.io/projects/dpi/dpi.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2019" target="_blank">ICLR 2019</a></b>,
				<a href="http://dpi.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/DPI-Net" target="_blank"> <small>[Code]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/dpi/dpi-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=FrPpP7aW3Lg" target="_blank"> <small>[Video]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.engadget.com/2019/04/21/mit-particle-simulator-helps-robots-make-sushi/" target="_blank"> <small>[Engadget]</small></a>
				<a href="https://news.developer.nvidia.com/laying-the-foundation-for-better-object-manipulation-in-robotics/" target="_blank"> <small>[NVIDIA Developer]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/infogail/infogail.png"/*tpa=https://yunzhuli.github.io/projects/infogail/infogail.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://tsong.me/" target="_blank">Jiaming Song</a>, and
				<a href="http://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon</a>
				<br>
				<b><font color="black">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</font></b><br>
				<b><a href="https://nips.cc/Conferences/2017" target="_blank">NIPS 2017</a></b>,
				<a href="https://yunzhuli.github.io/projects/infogail/infogail-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/InfoGAIL" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/infogail/infogail.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/infogail/infogail-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=YtNPBAW6h5k" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>

		</script>

		<script id="pubs_by_date" language="text">
		  <font color="black">(* indicates equal contribution)</font><br><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/d3fields/d3fields.gif"/*tpa=https://yunzhuli.github.io/projects/d3fields/d3fields.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="https://www.linkedin.com/in/zhuoran-li-david/" target="_blank">Zhuoran Li</a>*,
				<a href="https://robo-alex.github.io/" target="_blank">Mingtong Zhang</a>,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>
				<br>
				<b><font color="black">D<sup>3</sup>Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation</font></b><br>
				<b><a href="https://arxiv.org/abs/2309.16118" target="_blank">arXiv 2023</a></b>,
				<a href="https://robopil.github.io/d3fields/" target="_blank"> <small>[Project]</small></a>
				<a href="https://robopil.github.io/d3fields/d3fields.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/WangYixuan12/d3fields" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/d3fields/d3fields.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/RT-X/RT-X.jpg"/*tpa=https://yunzhuli.github.io/projects/RT-X/RT-X.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://robotics-transformer-x.github.io/" target="_blank">Open X-Embodiment Collaboration</a>
				<br>
				<b><font color="black">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</font></b><br>
				<b><a href="https://robotics-transformer-x.github.io/" target="_blank">Preprint 2023</a></b>,
				<a href="https://robotics-transformer-x.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://robotics-transformer-x.github.io/paper.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.deepmind.com/blog/scaling-up-learning-across-many-different-robot-types" target="_blank"> <small>[Blogpost]</small></a>
				<a href="https://console.cloud.google.com/storage/browser/gresearch/robotics/open_x_embodiment_and_rt_x_oss;tab=objects?prefix=&forceOnObjectsSortingFiltering=false" target="_blank"> <small>[Code]</small></a>
				<a href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit#gid=0" target="_blank"> <small>[Data]</small></a>
				<a href="https://yunzhuli.github.io/projects/RT-X/RT-X.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 1px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="https://yunzhuli.github.io/projects/robocook/spotlight_robocook.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://samuelpclarke.com/" target="_blank">Samuel Clarke</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://hshi74.github.io/robocook/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2306.14447" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/robocook" target="_blank"> <small>[Code]</small></a>
				<a href="https://hshi74.github.io/robocook/bibtex.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Best Systems Paper Award</b></font>
				</div>
			</div><hr>

			<div class="row">
			  <div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 1px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="https://yunzhuli.github.io/projects/VoxPoser/spotlight_voxposer.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank">Ruohan Zhang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>, and
				<a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a>
				<br>
				<b><font color="black">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://voxposer.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2307.05973" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=Yvn4eR05A3M" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/VoxPoser/VoxPoser.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stow/stow.jpg"/*tpa=https://yunzhuli.github.io/projects/stow/stow.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=SomOgNIAAAAJ&hl=en" target="_blank">Haonan Chen</a>,
				<a href="https://www.linkedin.com/in/yilong-niu-a13b5b289/" target="_blank">Yilong Niu</a>*,
				<a href="https://www.linkedin.com/in/kaiwen-hong-524520141/?locale=en_US" target="_blank">Kaiwen Hong</a>*,
				<a href="https://shuijing725.github.io/" target="_blank">Shuijing Liu</a>,
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>
				<br>
				<b><font color="black">Predicting Object Interactions with Behavior Primitives: An Application in Stowing Tasks</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://haonan16.github.io/stow_page/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2309.16873" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/haonan16/Stow/" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/stow/stow.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Finalist - Best Paper/Best Student Paper Awards</b></font>
				</div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dyn-res/dyn-res.gif"/*tpa=https://yunzhuli.github.io/projects/dyn-res/dyn-res.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Dynamic-Resolution Model Learning for Object Pile Manipulation</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2023</a></b>,
				<a href="https://robopil.github.io/dyn-res-pile-manip/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.16700" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/WangYixuan12/dyn-res-pile-manip" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/dyn-res/dyn-res.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Best Paper Award</b></font> at <b>IROS 2023</b> Workshop on Learning Meets Model-based Methods <a href="https://sites.google.com/view/learning-meets-models-iros2023" target="_blank"><small>[Link]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/robocraft-3d/robocraft-3d.gif"/*tpa=https://yunzhuli.github.io/projects/robocraft-3d/robocraft-3d.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects in 3D with Graph Networks</font></b><br>
				<b><a href="https://journals.sagepub.com/doi/10.1177/02783649231219020" target="_blank">IJRR 2023</a></b>,
				<a href="https://journals.sagepub.com/doi/10.1177/02783649231219020" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/robocraft3d" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/robocraft-3d/robocraft-3d.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/sparse-dynamics/sparse-dynamics.gif"/*tpa=https://yunzhuli.github.io/projects/sparse-dynamics/sparse-dynamics.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.ziangliu.com/" target="_blank">Ziang Liu</a>,
				<a href="https://www.linkedin.com/in/g9zhou" target="_blank">Genggeng Zhou</a>*,
				<a href="https://openreview.net/profile?id=~Jeff_He1" target="_blank">Jeff He</a>*,
				<a href="https://tobiamarcucci.github.io/" target="_blank">Tobia Marcucci</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a>, and
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>
				<br>
				<b><font color="black">Model-Based Control with Sparse Neural Dynamics</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b>,
				<a href="https://robopil.github.io/Sparse-Dynamics/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2312.12791" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/sparse-dynamics/sparse-dynamics.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/3d-intphys/3d-intphys.jpg"/*tpa=https://yunzhuli.github.io/projects/3d-intphys/3d-intphys.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xavihart.github.io/" target="_blank">Haotian Xue</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://sfish0101.bitbucket.io/" target="_blank">Hsiao-Yu Tung</a>
				<br>
				<b><font color="black">3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b>,
				<a href="https://openreview.net/forum?id=Fp5uC6YHwe" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/3d-intphys/3d-intphys.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

      		<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/actionosf/actionosf.jpg"/*tpa=https://yunzhuli.github.io/projects/actionosf/actionosf.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://s-tian.github.io/" target="_blank">Stephen Tian</a>*,
       			<a href="https://caiyancheng.github.io/academic.html" target="_blank">Yancheng Cai</a>*,
        		<a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
				<a href="https://zakharos.github.io/" target="_blank">Sergey Zakharov</a>,
        		<a href="https://www.thekatherineliu.com/" target="_blank">Katherine Liu</a>,
				<a href="https://adriengaidon.com/" target="_blank">Adrien Gaidon</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Multi-Object Manipulation via Object-Centric Neural Scattering Functions</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>,
				<a href="https://s-tian.github.io/projects/actionosf/", target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.08748", target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/actionosf/actionosf.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/objectfolderbenchmark/objectfolderbenchmark.jpg"/*tpa=https://yunzhuli.github.io/projects/objectfolderbenchmark/objectfolderbenchmark.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://ai.stanford.edu/~rhgao/" target="_blank">Ruohan Gao</a>*,
        <a href="https://dou-yiming.github.io/" target="_blank">Yiming Dou</a>*,
        <a href="https://haolirobo.github.io/" target="_blank">Hao Li</a>*,
				<a href="https://tanmay-agarwal.com/" target="_blank">Tanmay Agarwal</a>,
        <a href="http://web.stanford.edu/~bohg/" target="_blank">Jeannette Bohg</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
        <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>,
				<a href="https://objectfolder.stanford.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.00956" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.objectfolder.org/swan_vis/" target="_blank"> <small>[Demo]</small></a>
				<a href="https://yunzhuli.github.io/projects/objectfolderbenchmark/objectfolderbenchmark.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dec-ssl/dec-ssl.jpg"/*tpa=https://yunzhuli.github.io/projects/dec-ssl/dec-ssl.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.csail.mit.edu/person/lirui-wang" target="_blank">Lirui Wang</a>,
				<a href="https://kzhang66.github.io/" target="_blank">Kaiqing Zhang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://scholar.google.com/citations?user=OsP7JHAAAAAJ&hl=en" target="_blank">Yonglong Tian</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Does Learning from Decentralized Non-IID Unlabeled Data Benefit from Self Supervision?</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2023" target="_blank">ICLR 2023</a></b>,
				<a href="https://arxiv.org/abs/2210.10947" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/liruiw/Dec-SSL" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=CbSGwsihnEk" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/dec-ssl/dec-ssl.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

		  <div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comp_nerf_dy/combined.gif"/*tpa=https://yunzhuli.github.io/projects/comp_nerf_dy/combined.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://dannydriess.github.io/compnerfdyn/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=qUvTmyGpnm7" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/comp_nerf_dy/comp_nerf_dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/pasta/PASTA_bright.gif"/*tpa=https://yunzhuli.github.io/projects/pasta/PASTA_bright.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>*,
				<a href="https://www.linkedin.com/in/carlqi/" target="_blank">Carl Qi</a>*,
				<a href="https://yunchuzhang.github.io/" target="_blank">Yunchu Zhang</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://www.cs.cmu.edu/~katef/" target="_blank">Katerina Fragkiadaki</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>, and
				<a href="https://davheld.github.io/" target="_blank">David Held</a>
				<br>
				<b><font color="black">Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://sites.google.com/view/pasta-plan" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=tyxyBj2w4vw" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/pasta/pasta.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-rl/nerf-rl.gif"/*tpa=https://yunzhuli.github.io/projects/nerf-rl/nerf-rl.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://ingmarschubert.com/" target="_blank">Ingmar Schubert</a>,
				<a href="https://www.peteflorence.com/" target="_blank">Pete Florence</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Reinforcement Learning with Neural Radiance Fields</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a></b>,
				<a href="https://dannydriess.github.io/nerf-rl/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://dannydriess.github.io/papers/22-driess-NeRF-RL-preprint.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/nerf-rl/nerf-rl.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/ActionSense/ActionSense.jpg"/*tpa=https://yunzhuli.github.io/projects/ActionSense/ActionSense.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.josephdelpreto.com/" target="_blank">Joseph DelPreto</a>*,
				<a href="https://chaoliu.tech/" target="_blank">Chao Liu</a>*,
				<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
				<a href="https://danielarus.csail.mit.edu/" target="_blank">Daniela Rus</a>
				<br>
				<b><font color="black">ActionSense: A Multimodal Dataset and Recording Framework for Human Activities Using Wearable Sensors in a Kitchen Environment</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022 Datasets and Benchmarks</a></b>,
				<a href="https://action-net.csail.mit.edu/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=olvz0gAdGOX" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/ActionSense/ActionSense.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/robocraft/robocraft.gif"/*tpa=https://yunzhuli.github.io/projects/robocraft/robocraft.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2022</a></b>,
				<a href="http://hxu.rocks/robocraft/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2205.02909" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/RoboCraft" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/robocraft/robocraft.bib" target="_blank"> <small>[BibTex]</small></a><br>
				Abridged in <b>ICRA 2022</b> Workshop on Representing and Manipulating Deformable Objects <a href="https://deformable-workshop.github.io/icra2022/" target="_blank"><small>[Link]</small></a><br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robots-play-play-dough-0623" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.newscientist.com/article/2325970-ai-powered-robot-learned-to-make-letters-out-of-play-doh-on-its-own/" target="_blank"> <small>[NewScientist]</small></a>
				<a href="https://techcrunch.com/2022/06/23/a-quick-trip-to-mars/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh" target="_blank"> <small>[Stanford HAI]</small></a>

				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/TRO_multiobj_manip/TRO_multiobj_manip.jpg"/*tpa=https://yunzhuli.github.io/projects/TRO_multiobj_manip/TRO_multiobj_manip.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=0KfKHOsAAAAJ&hl=en" target="_blank">Zherong Pan</a>,
				<a href="https://andyzeng.github.io/" target="_blank">Andy Zeng</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://arc-l.github.io/" target="_blank">Jingjin Yu</a>, and
				<a href="https://kkhauser.web.illinois.edu/" target="_blank">Kris Hauser</a>
				<br>
				<b><font color="black">Algorithms and Systems for Manipulating Multiple Objects</font></b><br>
				<b><a href="https://www.ieee-ras.org/publications/t-ro" target="_blank">T-RO 2022</a></b>,
				<a href="https://ieeexplore.ieee.org/abstract/document/9893496" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/TRO_multiobj_manip/TRO_multiobj_manip.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/diffskill/DiffSkill-compressed.gif"/*tpa=https://yunzhuli.github.io/projects/diffskill/DiffSkill-compressed.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="https://davheld.github.io/" target="_blank">David Held</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://xingyu-lin.github.io/diffskill/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=Kef8cKdHWpP" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/diffskill/DiffSkill.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robotic-deformable-object-0331" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://techcrunch.com/2022/03/31/better-learning-through-complex-dough-manipulation/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://bdtechtalks.com/2022/05/09/diffskill-robotics-deformable-object-manipulation/" target="_blank"> <small>[TechTalks]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comphy/ComPhy.jpg"/*tpa=https://yunzhuli.github.io/projects/comphy/ComPhy.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://zfchenunique.github.io/" target="_blank">Zhenfang Chen</a>,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://dingmyu.github.io/" target="_blank">Mingyu Ding</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">ComPhy: Compositional Physical Reasoning of Objects and Events from Videos</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://comphyreasoning.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=PgNEYaIc81Q" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/comphy/ComPhy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/DAIS/DAIS.png"/*tpa=https://yunzhuli.github.io/projects/DAIS/DAIS.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.csail.mit.edu/person/lujie-yang" target="_blank">Lujie Yang</a>,
				<a href="https://kzhang66.github.io/" target="_blank">Kaiqing Zhang</a>,
				<a href="https://alexandreamice.github.io/" target="_blank">Alexandre Amice</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Discrete Approximate Information States in Partially Observable Environments</font></b><br>
				<b><a href="https://acc2022.a2c2.org/" target="_blank">ACC 2022</a></b>,
				<a href="https://yunzhuli.github.io/projects/DAIS/DAIS.pdf" target="_blank"> <small>[Paper]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-dy/nerf-dy-multiview.gif"/*tpa=https://yunzhuli.github.io/projects/nerf-dy/nerf-dy-multiview.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="https://people.csail.mit.edu/lishuang/" target="_blank">Shuang Li</a>*,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>,
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a></b>,
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="https://youtu.be/ELPMiifELGc" target="_blank"> <small>[Video]</small></a>
				<a href="https://openreview.net/forum?id=zv3NYgRZ7Qo" target="_blank"> <small>[OpenReview]</small></a>
				<a href="https://3d-representation-learning.github.io/nerf-dy/nerf-dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font><br>
				Abridged in <b>RSS 2021</b> Workshop on Visual Learning and Reasoning for Robotics <a href="https://rssvlrr.github.io/" target="_blank"><small>[Link]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/senstextile/senstextile.jpg"/*tpa=https://yunzhuli.github.io/projects/senstextile/senstextile.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="http://people.csail.mit.edu/kuiwu" target="_blank">Kui Wu</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/beichen-li" target="_blank">Beichen Li</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning Human-environment Interactions using Conformal Tactile Textiles</font></b><br>
				<b><a href="https://www.nature.com/natelectron/" target="_blank">Nature Electronics</a></b> 4, 193–201 (2021),
				<font color="firebrick"><b>5-year Impact Factor: 33.695</b></font>
				<br>
				<a href="http://senstextile.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41928-021-00558-0" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/senstextile" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/senstextile/senstextile.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Featured on the</small>
				<a href="https://www.nature.com/natelectron/volumes/4/issues/3" target="_blank"> <small>cover</small></a>
				<small>of the issue.</small>
				<small>Editorial comments</small>
				<a href="https://www.nature.com/articles/s41928-021-00567-z" target="_blank"> <small>[Link]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://www.nature.com/articles/s41928-021-00560-6" target="_blank"> <small>[Nature Electronics News & Views]</small></a>
				<a href="https://www.csail.mit.edu/news/smart-clothes-can-measure-your-movements" target="_blank"> <small>[MIT CSAIL News]</small></a>
				<a href="https://gizmodo.com/researchers-might-have-finally-cracked-smart-clothing-1846546202" target="_blank"> <small>[Gizmodo]</small></a>
				<a href="https://www.engadget.com/mit-csail-smart-clothes-track-movements-160010512.html" target="_blank"> <small>[Engadget]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/phystouch/phystouch.jpeg"/*tpa=https://yunzhuli.github.io/projects/phystouch/phystouch.jpeg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
					<a href="https://github.com/sjtuzq" target="_blank">Qiang Zhang</a>*,
					<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
					<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
					<a href="https://showone90.wixsite.com/show" target="_blank">Wan Shou</a>,
					<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
					<a href="https://thinklab.sjtu.edu.cn/" target="_blank">Junchi Yan</a>,
					<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>,
					<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
					<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
					<br>
					<b><font color="black">Dynamic Modeling of Hand-Object Interactions via Tactile Sensing</font></b><br>
					<b><a href="https://www.iros2021.org/" target="_blank">IROS 2021</a></b>,
					<a href="http://phystouch.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
					<a href="https://arxiv.org/abs/2109.04378" target="_blank"> <small>[Paper]</small></a>
					<a href="https://www.youtube.com/watch?v=rBN5kNOw5Y8" target="_blank"> <small>[Video]</small></a>
					<a href="https://yunzhuli.github.io/projects/phystouch/phystouch.bib" target="_blank"> <small>[BibTex]</small></a>
					<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/intcarpet/logo.jpg"/*tpa=https://yunzhuli.github.io/projects/intcarpet/logo.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Intelligent Carpet: Inferring 3D Human Pose from Tactile Signals</font></b><br>
				<b><a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a></b>,
				<a href="http://intcarpet.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Intelligent_Carpet_Inferring_3D_Human_Pose_From_Tactile_Signals_CVPR_2021_paper.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yiyueluo/IntelligentCarpet" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=U6svj37h2U4" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/intcarpet/intcarpet.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2021/intelligent-carpet-gives-insight-human-poses-0624" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.fastcompany.com/90648670/this-magic-carpet-can-track-your-workout" target="_blank"> <small>[Fast Company]</small></a>
				<br>
				</div>
			</div><hr>
			
			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/v-cdn/v-cdn.gif"/*tpa=https://yunzhuli.github.io/projects/v-cdn/v-cdn.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Animashree Anandkumar</a>,
				<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, and
				<a href="https://animesh.garg.tech/" target="_blank">Animesh Garg</a>
				<br>
				<b><font color="black">Causal Discovery in Physical Systems from Videos</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://yunzhuli.github.io/www/V-CDN/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2007.00631" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/pairlab/v-cdn" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=hRsCt8xLn_8" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/poster.pdf" target="_blank"> <small>[Poster]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/V-CDN.bib" target="_blank"> <small>[BibTex]</small></a>
			  <br>	
				<small>Covered by</small>
				<a href="https://venturebeat.com/2020/07/02/ai-system-learns-to-model-how-fabrics-interact-by-watching-videos/" target="_blank"> <small>[VentureBeat]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/physical_scene_graphs/physical_scene_graphs.jpg"/*tpa=https://yunzhuli.github.io/projects/physical_scene_graphs/physical_scene_graphs.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="https://scholar.google.com/citations?user=YM4x068AAAAJ&hl=en" target="_blank">Chaofei Fan</a>,
				<a href="https://scholar.google.com/citations?user=GADXPDcAAAAJ&hl=en" target="_blank">Damian Mrowca</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://bcs.mit.edu/users/altersmitedu" target="_blank">Seth Alter</a>,
				<a href="https://sites.google.com/site/anayebihomepage/" target="_blank">Aran Nayebi</a>,
				<a href="https://bcs-r1.mit.edu/users/jeremyesmitedu" target="_blank">Jeremy Schwartz</a>,
				<a href="http://vision.stanford.edu/people.html" target="_blank">Li Fei-Fei</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>
				<br>
				<b><font color="black">Learning Physical Graph Representations from Visual Scenes</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://neuroailab.github.io/physical-scene-graphs/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2006.12373" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/physical_scene_graphs/physical_scene_graphs.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/key_dynam/key_dynam.gif"/*tpa=https://yunzhuli.github.io/projects/key_dynam/key_dynam.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="http://lucasmanuelli.com/" target="_blank">Lucas Manuelli</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://www.peteflorence.com/" target="_blank">Pete Florence</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Keypoints into the Future: Self-Supervised Correspondence in Model-Based Reinforcement Learning</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2020</a></b>,
				<a href="https://sites.google.com/view/keypointsintothefuture" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2009.05085" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=qxC7XS4eFFw" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/key_dynam/key_dynam.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visual_grounding/visual_grounding.gif"/*tpa=https://yunzhuli.github.io/projects/visual_grounding/visual_grounding.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://scholar.google.com/citations?user=Rxb7o6IAAAAJ&hl=en" target="_blank">Toru Lin</a>*,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Visual Grounding of Learned Physical Models</font></b><br>
				<b><a href="https://icml.cc/Conferences/2020" target="_blank">ICML 2020</a></b>,
				<a href="http://visual-physics-grounding.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2004.13664" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/VGPL" target="_blank"><small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/visual_grounding/vgpl.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/compkpm/compkpm.gif"/*tpa=https://yunzhuli.github.io/projects/compkpm/compkpm.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>*,
				<a href="http://people.csail.mit.edu/hehaodele/" target="_blank">Hao He</a>*,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/dina/" target="_blank">Dina Katabi</a>, and
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Compositional Koopman Operators for Model-Based Control</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://koopman.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=H1ldzA4tPr" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/CompositionalKoopmanOperators" target="_blank"><small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/compkpm/compkpm.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://youtu.be/MnXo_hjh1Q4" target="_blank"> <small>[Video]</small></a>
				<a href="http://koopman.csail.mit.edu/poster.pdf" target="_blank"> <small>[Poster]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation</b></font><br>
				Abridged in <b>NeurIPS 2019</b> Workshop on Graph Representation Learning <a href="https://grlearning.github.io/papers/" target="_blank"><small>[Link]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/clevrer/clevrer.gif"/*tpa=https://yunzhuli.github.io/projects/clevrer/clevrer.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>*,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://sites.google.com/site/pushmeet/" target="_blank">Pushmeet Kohli</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>
				<br>
				<b><font color="black">CLEVRER: Collision Events for Video Representation and Reasoning</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://clevrer.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=HkxYzANYDB" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/clevrer/clevrer.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation</b></font><br>

				<small>Covered by</small>
				<a href="https://www.youtube.com/watch?v=bVXPnP8k6yo" target="_blank"> <small>[Two Minute Papers]</small></a>
				<a href="https://www.technologyreview.com/2020/03/06/905479/ai-neuro-symbolic-system-reasons-like-child-deepmind-ibm-mit/" target="_blank"> <small>[MIT Technology Review]</small></a>
				<a href="https://www.wired.com/story/ai-smart-cant-grasp-cause-effect/" target="_blank"> <small>[WIRED]</small></a>
				<a href="https://venturebeat.com/2020/02/26/researchers-apply-developmental-psychology-to-ai-model-that-predicts-object-relationships/" target="_blank"> <small>[VentureBeat]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stag/stag_lowres.jpg"/*tpa=https://yunzhuli.github.io/projects/stag/stag_lowres.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/subras/" target="_blank">Subramanian Sundaram</a>,
				<a href="https://people.csail.mit.edu/pkellnho/" target="_blank">Petr Kellnhofer</a>,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://people.csail.mit.edu/wojciech/" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning the Signatures of the Human Grasp Using a Scalable Tactile Glove</font></b><br>
				<b><a href="https://www.nature.com/" target="_blank">Nature</a></b> 569, 698–702 (2019),
				<font color="firebrick"><b>5-year Impact Factor: 54.637</b></font>
				<br>
				<a href="http://stag.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41586-019-1234-z" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/Erkil1452/touch" target="_blank"> <small>[Code]</small></a>
				<a href="http://stag.csail.mit.edu/files/sundaram2019stag.bib" target="_blank"> <small>[BibTex]</small></a>,

				<small>Collected by</small>
				<a href="projects/stag/stag_mit_museum.jpg"/*tpa=https://yunzhuli.github.io/projects/stag/stag_mit_museum.jpg*/ target="_blank"> <small>MIT Museum</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.nature.com/articles/d41586-019-01593-w" target="_blank"> <small>[Nature News & Views]</small></a>
				<a href="https://devicematerialscommunity.nature.com/users/257334-subramanian-sundaram/posts/49420-learning-dexterity-from-humans" target="_blank"> <small>[Nature communities]</small></a>
				<a href="https://www.economist.com/science-and-technology/2019/05/30/improving-robots-grasp-requires-a-new-way-to-measure-it-in-humans" target="_blank"> <small>[The Economist]</small></a>
				<a href="https://www.pbs.org/wgbh/nova/article/electronic-glove-pressure-sensors/" target="_blank"> <small>[PBS NOVA]</small></a>
				<a href="https://www.bbc.co.uk/sounds/play/p079yr9y" target="_blank"> <small>[BBC Radio]</small></a>
				<a href="https://www.newscientist.com/article/2204736-smart-glove-works-out-what-youre-holding-from-its-weight-and-shape/" target="_blank"> <small>[NewScientist]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visgel/visgel.jpg"/*tpa=https://yunzhuli.github.io/projects/visgel/visgel.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Connecting Touch and Vision via Cross-Modal Prediction</font></b><br>
				<b><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a></b>,
				<a href="http://visgel.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/VisGel" target="_blank"><small>[Code]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel.bib" target="_blank"> <small>[BibTex]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/teaching-ai-to-connect-senses-vision-touch-0617" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.bbc.com/news/av/technology-48711479/robot-taught-to-feel-objects-by-sight-and-other-news" target="_blank"> <small>[BBC]</small></a>
				<a href="https://www.cnn.com/2019/06/17/us/mit-robot-vision-touch-trnd/index.html" target="_blank"> <small>[CNN]</small></a>
				<a href="https://www.forbes.com/sites/charlestowersclark/2019/06/17/one-step-closer-to-human-intelligence-mit-csail-combine-sight-and-touch-in-ai/#3496256578b6" target="_blank"> <small>[Forbes]</small></a>
				<a href="https://techcrunch.com/2019/06/17/mit-develops-a-system-to-give-robots-more-human-senses/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://www.fastcompany.com/90365007/a-new-robot-can-now-identify-objects-by-touch" target="_blank"> <small>[Fast Company]</small></a>
				<a href="https://www.engadget.com/2019/06/17/robot-identify-sight-touch/" target="_blank"> <small>[Engadget]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dpi/dpi.png"/*tpa=https://yunzhuli.github.io/projects/dpi/dpi.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2019" target="_blank">ICLR 2019</a></b>,
				<a href="http://dpi.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/DPI-Net" target="_blank"> <small>[Code]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/dpi/dpi-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=FrPpP7aW3Lg" target="_blank"> <small>[Video]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.engadget.com/2019/04/21/mit-particle-simulator-helps-robots-make-sushi/" target="_blank"> <small>[Engadget]</small></a>
				<a href="https://news.developer.nvidia.com/laying-the-foundation-for-better-object-manipulation-in-robotics/" target="_blank"> <small>[NVIDIA Developer]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/propnet/propnet-1.png"/*tpa=https://yunzhuli.github.io/projects/propnet/propnet-1.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Propagation Networks for Model-Based Control Under Partial Observation</font></b><br>
				<b><a href="https://www.icra2019.org/" target="_blank">ICRA 2019</a></b>,
				<a href="http://propnet.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/PropNet" target="_blank"><small>[Code]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://www.youtube.com/watch?v=ZAxHXegkz48" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/infogail/infogail.png"/*tpa=https://yunzhuli.github.io/projects/infogail/infogail.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://tsong.me/" target="_blank">Jiaming Song</a>, and
				<a href="http://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon</a>
				<br>
				<b><font color="black">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</font></b><br>
				<b><a href="https://nips.cc/Conferences/2017" target="_blank">NIPS 2017</a></b>,
				<a href="https://yunzhuli.github.io/projects/infogail/infogail-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/InfoGAIL" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/infogail/infogail.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/infogail/infogail-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=YtNPBAW6h5k" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/skin/detection_tracking.png"/*tpa=https://yunzhuli.github.io/projects/skin/detection_tracking.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="http://cs.stanford.edu/people/esteva/home/index.html" target="_blank">Andre Esteva</a>*,
				<a href="https://stanford.edu/~kuprel/" target="_blank">Brett Kuprel</a>,
				<a href="https://profiles.stanford.edu/roberto-novoa" target="_blank">Rob Novoa</a>,
				<a href="https://profiles.stanford.edu/justin-ko" target="_blank">Justin Ko</a>, and
				<a href="http://robots.stanford.edu/" target="_blank">Sebastian Thrun</a>
				<br>
				<b><font color="black">Skin Cancer Detection and Tracking using Data Synthesis and Deep Learning</font></b><br>
				<a href="https://nips.cc/Conferences/2016" target="_blank"><b>NIPS 2016</b> Workshop on Machine Learning for Health</a><br>
				<a href="http://w3phiai2017.w3phi.com/" target="_blank"><b>AAAI 2017</b> Joint Workshop on Health Intelligence</a><br>
				<a href="https://yunzhuli.github.io/projects/skin/detection_tracking-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://yunzhuli.github.io/projects/skin/skin.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/conv3d/face_detection_conv3d.png"/*tpa=https://yunzhuli.github.io/projects/conv3d/face_detection_conv3d.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				Benyuan Sun*,
				<a href="http://www.stat.ucla.edu/~tfwu/" target="_blank">Tianfu Wu</a>, and
				<a href="http://www.idm.pku.edu.cn/staff/wangyizhou/" target="_blank">Yizhou Wang</a>
				<br>
				<b><font color="black">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</font></b><br>
				<b><a href="http://www.eccv2016.org/" target="_blank">ECCV 2016</a></b>,
				<a href="https://yunzhuli.github.io/projects/conv3d/face_detection_conv3d-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/tfwu/FaceDetection-ConvNet-3D" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/conv3d/conv3d.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/conv3d/face_detection_conv3d-poster.pdf" target="_blank"> <small>[Poster]</small></a>
				</div>
			</div><hr>

		</script>


		<script id="pubs_by_topic" language="text">
		
			<font color="black">(* indicates equal contribution)</font><br><hr>

			<div id="manip" style="padding-top: 80px; margin-top: -80px;">
			  <h5>Robotic Manipulation</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/d3fields/d3fields.gif"/*tpa=https://yunzhuli.github.io/projects/d3fields/d3fields.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="https://www.linkedin.com/in/zhuoran-li-david/" target="_blank">Zhuoran Li</a>*,
				<a href="https://robo-alex.github.io/" target="_blank">Mingtong Zhang</a>,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>
				<br>
				<b><font color="black">D<sup>3</sup>Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation</font></b><br>
				<b><a href="https://arxiv.org/abs/2309.16118" target="_blank">arXiv 2023</a></b>,
				<a href="https://robopil.github.io/d3fields/" target="_blank"> <small>[Project]</small></a>
				<a href="https://robopil.github.io/d3fields/d3fields.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/WangYixuan12/d3fields" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/d3fields/d3fields.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/RT-X/RT-X.jpg"/*tpa=https://yunzhuli.github.io/projects/RT-X/RT-X.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://robotics-transformer-x.github.io/" target="_blank">Open X-Embodiment Collaboration</a>
				<br>
				<b><font color="black">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</font></b><br>
				<b><a href="https://robotics-transformer-x.github.io/" target="_blank">Preprint 2023</a></b>,
				<a href="https://robotics-transformer-x.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://robotics-transformer-x.github.io/paper.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.deepmind.com/blog/scaling-up-learning-across-many-different-robot-types" target="_blank"> <small>[Blogpost]</small></a>
				<a href="https://console.cloud.google.com/storage/browser/gresearch/robotics/open_x_embodiment_and_rt_x_oss;tab=objects?prefix=&forceOnObjectsSortingFiltering=false" target="_blank"> <small>[Code]</small></a>
				<a href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit#gid=0" target="_blank"> <small>[Data]</small></a>
				<a href="https://yunzhuli.github.io/projects/RT-X/RT-X.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 1px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="https://yunzhuli.github.io/projects/robocook/spotlight_robocook.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://samuelpclarke.com/" target="_blank">Samuel Clarke</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://hshi74.github.io/robocook/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2306.14447" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/robocook" target="_blank"> <small>[Code]</small></a>
				<a href="https://hshi74.github.io/robocook/bibtex.txt" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Best Systems Paper Award</b></font>
				</div>
			</div><hr>

			<div class="row">
			  <div class="col-md-3">
					<div class="video-container" style="overflow: hidden;">
						<video class="img_responsive" style="border: 1px solid black; width: 100%; display: block; height: auto;" autoplay muted loop webkit-playsinline playsinline>
							<source src="https://yunzhuli.github.io/projects/VoxPoser/spotlight_voxposer.mp4" type="video/mp4">
						</video>
					</div>
				</div>
				<div class="col-md-9">
				<a href="https://wenlong.page/" target="_blank">Wenlong Huang</a>,
				<a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a>,
				<a href="https://ai.stanford.edu/~zharu/" target="_blank">Ruohan Zhang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>, and
				<a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a>
				<br>
				<b><font color="black">VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://voxposer.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2307.05973" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=Yvn4eR05A3M" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/VoxPoser/VoxPoser.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stow/stow.jpg"/*tpa=https://yunzhuli.github.io/projects/stow/stow.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=SomOgNIAAAAJ&hl=en" target="_blank">Haonan Chen</a>,
				<a href="https://www.linkedin.com/in/yilong-niu-a13b5b289/" target="_blank">Yilong Niu</a>*,
				<a href="https://www.linkedin.com/in/kaiwen-hong-524520141/?locale=en_US" target="_blank">Kaiwen Hong</a>*,
				<a href="https://shuijing725.github.io/" target="_blank">Shuijing Liu</a>,
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>
				<br>
				<b><font color="black">Predicting Object Interactions with Behavior Primitives: An Application in Stowing Tasks</font></b><br>
				<b><a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a></b>,
				<a href="https://haonan16.github.io/stow_page/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2309.16873" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/haonan16/Stow/" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/stow/stow.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Finalist - Best Paper/Best Student Paper Awards</b></font>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dyn-res/dyn-res.gif"/*tpa=https://yunzhuli.github.io/projects/dyn-res/dyn-res.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang</a>*,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="https://krdc.web.illinois.edu/" target="_blank">Katherine Driggs-Campbell</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Dynamic-Resolution Model Learning for Object Pile Manipulation</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2023</a></b>,
				<a href="https://robopil.github.io/dyn-res-pile-manip/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.16700" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/WangYixuan12/dyn-res-pile-manip" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/dyn-res/dyn-res.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Best Paper Award</b></font> at <b>IROS 2023</b> Workshop on Learning Meets Model-based Methods <a href="https://sites.google.com/view/learning-meets-models-iros2023" target="_blank"><small>[Link]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/robocraft-3d/robocraft-3d.gif"/*tpa=https://yunzhuli.github.io/projects/robocraft-3d/robocraft-3d.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects in 3D with Graph Networks</font></b><br>
				<b><a href="https://journals.sagepub.com/doi/10.1177/02783649231219020" target="_blank">IJRR 2023</a></b>,
				<a href="https://journals.sagepub.com/doi/10.1177/02783649231219020" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/robocraft3d" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/robocraft-3d/robocraft-3d.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/sparse-dynamics/sparse-dynamics.gif"/*tpa=https://yunzhuli.github.io/projects/sparse-dynamics/sparse-dynamics.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.ziangliu.com/" target="_blank">Ziang Liu</a>,
				<a href="https://www.linkedin.com/in/g9zhou" target="_blank">Genggeng Zhou</a>*,
				<a href="https://openreview.net/profile?id=~Jeff_He1" target="_blank">Jeff He</a>*,
				<a href="https://tobiamarcucci.github.io/" target="_blank">Tobia Marcucci</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a>, and
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>
				<br>
				<b><font color="black">Model-Based Control with Sparse Neural Dynamics</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b>,
				<a href="https://robopil.github.io/Sparse-Dynamics/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2312.12791" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/sparse-dynamics/sparse-dynamics.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/actionosf/actionosf.jpg"/*tpa=https://yunzhuli.github.io/projects/actionosf/actionosf.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://s-tian.github.io/" target="_blank">Stephen Tian</a>*,
        <a href="https://caiyancheng.github.io/academic.html" target="_blank">Yancheng Cai</a>*,
        <a href="https://kovenyu.com/" target="_blank">Hong-Xing Yu</a>,
				<a href="https://zakharos.github.io/" target="_blank">Sergey Zakharov</a>,
        <a href="https://www.thekatherineliu.com/" target="_blank">Katherine Liu</a>,
				<a href="https://adriengaidon.com/" target="_blank">Adrien Gaidon</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">Multi-Object Manipulation via Object-Centric Neural Scattering Functions</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>,
				<a href="https://s-tian.github.io/projects/actionosf/", target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.08748", target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/actionosf/actionosf.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comp_nerf_dy/combined.gif"/*tpa=https://yunzhuli.github.io/projects/comp_nerf_dy/combined.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://dannydriess.github.io/compnerfdyn/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=qUvTmyGpnm7" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/comp_nerf_dy/comp_nerf_dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/pasta/PASTA_bright.gif"/*tpa=https://yunzhuli.github.io/projects/pasta/PASTA_bright.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>*,
				<a href="https://www.linkedin.com/in/carlqi/" target="_blank">Carl Qi</a>*,
				<a href="https://yunchuzhang.github.io/" target="_blank">Yunchu Zhang</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="https://www.cs.cmu.edu/~katef/" target="_blank">Katerina Fragkiadaki</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>, and
				<a href="https://davheld.github.io/" target="_blank">David Held</a>
				<br>
				<b><font color="black">Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation</font></b><br>
				<b><a href="https://corl2022.org/" target="_blank">CoRL 2022</a></b>,
				<a href="https://sites.google.com/view/pasta-plan" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=tyxyBj2w4vw" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/pasta/pasta.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-rl/nerf-rl.gif"/*tpa=https://yunzhuli.github.io/projects/nerf-rl/nerf-rl.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://dannydriess.github.io/" target="_blank">Danny Driess</a>,
				<a href="https://ingmarschubert.com/" target="_blank">Ingmar Schubert</a>,
				<a href="https://www.peteflorence.com/" target="_blank">Pete Florence</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://www.user.tu-berlin.de/mtoussai/" target="_blank">Marc Toussaint</a>
				<br>
				<b><font color="black">Reinforcement Learning with Neural Radiance Fields</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a></b>,
				<a href="https://dannydriess.github.io/nerf-rl/index.html" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://dannydriess.github.io/papers/22-driess-NeRF-RL-preprint.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/nerf-rl/nerf-rl.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/robocraft/robocraft.gif"/*tpa=https://yunzhuli.github.io/projects/robocraft/robocraft.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a>*,
				<a href="http://hxu.rocks/" target="_blank">Huazhe Xu</a>*,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">RoboCraft: Learning to See, Simulate, and Shape Elasto-Plastic Objects with Graph Networks</font></b><br>
				<b><a href="https://roboticsconference.org/" target="_blank">RSS 2022</a></b>,
				<a href="http://hxu.rocks/robocraft/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://arxiv.org/abs/2205.02909" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/hshi74/RoboCraft" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/robocraft/robocraft.bib" target="_blank"> <small>[BibTex]</small></a><br>
				Abridged in <b>ICRA 2022</b> Workshop on Representing and Manipulating Deformable Objects <a href="https://deformable-workshop.github.io/icra2022/" target="_blank"><small>[Link]</small></a><br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robots-play-play-dough-0623" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.newscientist.com/article/2325970-ai-powered-robot-learned-to-make-letters-out-of-play-doh-on-its-own/" target="_blank"> <small>[NewScientist]</small></a>
				<a href="https://techcrunch.com/2022/06/23/a-quick-trip-to-mars/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://hai.stanford.edu/news/training-robot-shape-letters-play-doh" target="_blank"> <small>[Stanford HAI]</small></a>

				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/TRO_multiobj_manip/TRO_multiobj_manip.jpg"/*tpa=https://yunzhuli.github.io/projects/TRO_multiobj_manip/TRO_multiobj_manip.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=0KfKHOsAAAAJ&hl=en" target="_blank">Zherong Pan</a>,
				<a href="https://andyzeng.github.io/" target="_blank">Andy Zeng</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://arc-l.github.io/" target="_blank">Jingjin Yu</a>, and
				<a href="https://kkhauser.web.illinois.edu/" target="_blank">Kris Hauser</a>
				<br>
				<b><font color="black">Algorithms and Systems for Manipulating Multiple Objects</font></b><br>
				<b><a href="https://www.ieee-ras.org/publications/t-ro" target="_blank">T-RO 2022</a></b>,
				<a href="https://ieeexplore.ieee.org/abstract/document/9893496" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/TRO_multiobj_manip/TRO_multiobj_manip.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/diffskill/DiffSkill-compressed.gif"/*tpa=https://yunzhuli.github.io/projects/diffskill/DiffSkill-compressed.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xingyu-lin.github.io/" target="_blank">Xingyu Lin</a>,
				<a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="https://davheld.github.io/" target="_blank">David Held</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://xingyu-lin.github.io/diffskill/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=Kef8cKdHWpP" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/diffskill/DiffSkill.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2022/robotic-deformable-object-0331" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://techcrunch.com/2022/03/31/better-learning-through-complex-dough-manipulation/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://bdtechtalks.com/2022/05/09/diffskill-robotics-deformable-object-manipulation/" target="_blank"> <small>[TechTalks]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/DAIS/DAIS.png"/*tpa=https://yunzhuli.github.io/projects/DAIS/DAIS.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.csail.mit.edu/person/lujie-yang" target="_blank">Lujie Yang</a>,
				<a href="https://kzhang66.github.io/" target="_blank">Kaiqing Zhang</a>,
				<a href="https://alexandreamice.github.io/" target="_blank">Alexandre Amice</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Discrete Approximate Information States in Partially Observable Environments</font></b><br>
				<b><a href="https://acc2022.a2c2.org/" target="_blank">ACC 2022</a></b>,
				<a href="https://yunzhuli.github.io/projects/DAIS/DAIS.pdf" target="_blank"> <small>[Paper]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/nerf-dy/nerf-dy-multiview.gif"/*tpa=https://yunzhuli.github.io/projects/nerf-dy/nerf-dy-multiview.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="https://people.csail.mit.edu/lishuang/" target="_blank">Shuang Li</a>*,
				<a href="https://vsitzmann.github.io/" target="_blank">Vincent Sitzmann</a>,
				<a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">3D Neural Scene Representations for Visuomotor Control</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2021</a></b>,
				<a href="https://3d-representation-learning.github.io/nerf-dy/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2107.04004" target="_blank"> <small>[Paper]</small></a>
				<a href="https://youtu.be/ELPMiifELGc" target="_blank"> <small>[Video]</small></a>
				<a href="https://openreview.net/forum?id=zv3NYgRZ7Qo" target="_blank"> <small>[OpenReview]</small></a>
				<a href="https://3d-representation-learning.github.io/nerf-dy/nerf-dy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font><br>
				Abridged in <b>RSS 2021</b> Workshop on Visual Learning and Reasoning for Robotics <a href="https://rssvlrr.github.io/" target="_blank"><small>[Link]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/key_dynam/key_dynam.gif"/*tpa=https://yunzhuli.github.io/projects/key_dynam/key_dynam.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="http://lucasmanuelli.com/" target="_blank">Lucas Manuelli</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://www.peteflorence.com/" target="_blank">Pete Florence</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Keypoints into the Future: Self-Supervised Correspondence in Model-Based Reinforcement Learning</font></b><br>
				<b><a href="https://www.robot-learning.org/" target="_blank">CoRL 2020</a></b>,
				<a href="https://sites.google.com/view/keypointsintothefuture" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2009.05085" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.youtube.com/watch?v=qxC7XS4eFFw" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/key_dynam/key_dynam.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/compkpm/compkpm.gif"/*tpa=https://yunzhuli.github.io/projects/compkpm/compkpm.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>*,
				<a href="http://people.csail.mit.edu/hehaodele/" target="_blank">Hao He</a>*,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/dina/" target="_blank">Dina Katabi</a>, and
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Compositional Koopman Operators for Model-Based Control</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://koopman.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=H1ldzA4tPr" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/CompositionalKoopmanOperators" target="_blank"><small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/compkpm/compkpm.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://youtu.be/MnXo_hjh1Q4" target="_blank"> <small>[Video]</small></a>
				<a href="http://koopman.csail.mit.edu/poster.pdf" target="_blank"> <small>[Poster]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation</b></font><br>
				Abridged in <b>NeurIPS 2019</b> Workshop on Graph Representation Learning <a href="https://grlearning.github.io/papers/" target="_blank"><small>[Link]</small></a>
				
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/dpi/dpi.png"/*tpa=https://yunzhuli.github.io/projects/dpi/dpi.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2019" target="_blank">ICLR 2019</a></b>,
				<a href="http://dpi.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/DPI-Net" target="_blank"> <small>[Code]</small></a>
				<a href="http://dpi.csail.mit.edu/dpi.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/dpi/dpi-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=FrPpP7aW3Lg" target="_blank"> <small>[Video]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/robots-object-manipulation-particle-simulator-0417" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.engadget.com/2019/04/21/mit-particle-simulator-helps-robots-make-sushi/" target="_blank"> <small>[Engadget]</small></a>
				<a href="https://news.developer.nvidia.com/laying-the-foundation-for-better-object-manipulation-in-robotics/" target="_blank"> <small>[NVIDIA Developer]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/propnet/propnet-1.png"/*tpa=https://yunzhuli.github.io/projects/propnet/propnet-1.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>
				<br>
				<b><font color="black">Propagation Networks for Model-Based Control Under Partial Observation</font></b><br>
				<b><a href="https://www.icra2019.org/" target="_blank">ICRA 2019</a></b>,
				<a href="http://propnet.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/PropNet" target="_blank"><small>[Code]</small></a>
				<a href="http://propnet.csail.mit.edu/propnet.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://www.youtube.com/watch?v=ZAxHXegkz48" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>


			<br>
			<div id="phys" style="padding-top: 80px; margin-top: -80px;">
				<h5>Physical Scene Understanding</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/3d-intphys/3d-intphys.jpg"/*tpa=https://yunzhuli.github.io/projects/3d-intphys/3d-intphys.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://xavihart.github.io/" target="_blank">Haotian Xue</a>,
				<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>,
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>, and
				<a href="https://sfish0101.bitbucket.io/" target="_blank">Hsiao-Yu Tung</a>
				<br>
				<b><font color="black">3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a></b>,
				<a href="https://openreview.net/forum?id=Fp5uC6YHwe" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/3d-intphys/3d-intphys.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

   			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/comphy/ComPhy.jpg"/*tpa=https://yunzhuli.github.io/projects/comphy/ComPhy.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://zfchenunique.github.io/" target="_blank">Zhenfang Chen</a>,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://dingmyu.github.io/" target="_blank">Mingyu Ding</a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>
				<br>
				<b><font color="black">ComPhy: Compositional Physical Reasoning of Objects and Events from Videos</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a></b>,
				<a href="https://comphyreasoning.github.io/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=PgNEYaIc81Q" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/comphy/ComPhy.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/v-cdn/v-cdn.gif"/*tpa=https://yunzhuli.github.io/projects/v-cdn/v-cdn.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
				<a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Animashree Anandkumar</a>,
				<a href="https://homes.cs.washington.edu/~fox/" target="_blank">Dieter Fox</a>, and
				<a href="https://animesh.garg.tech/" target="_blank">Animesh Garg</a>
				<br>
				<b><font color="black">Causal Discovery in Physical Systems from Videos</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://yunzhuli.github.io/www/V-CDN/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2007.00631" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/pairlab/v-cdn" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=hRsCt8xLn_8" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/poster.pdf" target="_blank"> <small>[Poster]</small></a>
				<a href="https://yunzhuli.github.io/www/V-CDN/V-CDN.bib" target="_blank"> <small>[BibTex]</small></a>
			  <br>	
				<small>Covered by</small>
				<a href="https://venturebeat.com/2020/07/02/ai-system-learns-to-model-how-fabrics-interact-by-watching-videos/" target="_blank"> <small>[VentureBeat]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/physical_scene_graphs/physical_scene_graphs.jpg"/*tpa=https://yunzhuli.github.io/projects/physical_scene_graphs/physical_scene_graphs.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="https://scholar.google.com/citations?user=YM4x068AAAAJ&hl=en" target="_blank">Chaofei Fan</a>,
				<a href="https://scholar.google.com/citations?user=GADXPDcAAAAJ&hl=en" target="_blank">Damian Mrowca</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://bcs.mit.edu/users/altersmitedu" target="_blank">Seth Alter</a>,
				<a href="https://sites.google.com/site/anayebihomepage/" target="_blank">Aran Nayebi</a>,
				<a href="https://bcs-r1.mit.edu/users/jeremyesmitedu" target="_blank">Jeremy Schwartz</a>,
				<a href="http://vision.stanford.edu/people.html" target="_blank">Li Fei-Fei</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>
				<br>
				<b><font color="black">Learning Physical Graph Representations from Visual Scenes</font></b><br>
				<b><a href="https://nips.cc/Conferences/2020" target="_blank">NeurIPS 2020</a></b>,
				<a href="https://neuroailab.github.io/physical-scene-graphs/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2006.12373" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/physical_scene_graphs/physical_scene_graphs.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Oral Presentation</b></font><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visual_grounding/visual_grounding.gif"/*tpa=https://yunzhuli.github.io/projects/visual_grounding/visual_grounding.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="https://scholar.google.com/citations?user=Rxb7o6IAAAAJ&hl=en" target="_blank">Toru Lin</a>*,
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="https://scholar.google.com/citations?user=uYbkEzYAAAAJ&hl=en" target="_blank">Daniel M. Bear</a>,
				<a href="http://stanford.edu/~yamins/" target="_blank">Daniel L. K. Yamins</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Visual Grounding of Learned Physical Models</font></b><br>
				<b><a href="https://icml.cc/Conferences/2020" target="_blank">ICML 2020</a></b>,
				<a href="http://visual-physics-grounding.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2004.13664" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/VGPL" target="_blank"><small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/visual_grounding/vgpl.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/clevrer/clevrer.gif"/*tpa=https://yunzhuli.github.io/projects/clevrer/clevrer.gif*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://scholar.google.com/citations?user=SwxS_JkAAAAJ&hl=en" target="_blank">Kexin Yi</a>*,
				<a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>*,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://sites.google.com/site/pushmeet/" target="_blank">Pushmeet Kohli</a>,
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://web.mit.edu/cocosci/josh.html" target="_blank">Joshua B. Tenenbaum</a>
				<br>
				<b><font color="black">CLEVRER: Collision Events for Video Representation and Reasoning</font></b><br>
				<b><a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a></b>,
				<a href="http://clevrer.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openreview.net/forum?id=HkxYzANYDB" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/clevrer/clevrer.bib" target="_blank"> <small>[BibTex]</small></a><br>
				<font color="firebrick"><b>Spotlight Presentation</b></font><br>

				<small>Covered by</small>
				<a href="https://www.youtube.com/watch?v=bVXPnP8k6yo" target="_blank"> <small>[Two Minute Papers]</small></a>
				<a href="https://www.technologyreview.com/2020/03/06/905479/ai-neuro-symbolic-system-reasons-like-child-deepmind-ibm-mit/" target="_blank"> <small>[MIT Technology Review]</small></a>
				<a href="https://www.wired.com/story/ai-smart-cant-grasp-cause-effect/" target="_blank"> <small>[WIRED]</small></a>
				<a href="https://venturebeat.com/2020/02/26/researchers-apply-developmental-psychology-to-ai-model-that-predicts-object-relationships/" target="_blank"> <small>[VentureBeat]</small></a>
				
				</div>
			</div><hr>


			<br>
			<div id="multi" style="padding-top: 80px; margin-top: -80px;">
				<h5>Multi-Modal Perception</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/objectfolderbenchmark/objectfolderbenchmark.jpg"/*tpa=https://yunzhuli.github.io/projects/objectfolderbenchmark/objectfolderbenchmark.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://ai.stanford.edu/~rhgao/" target="_blank">Ruohan Gao</a>*,
        <a href="https://dou-yiming.github.io/" target="_blank">Yiming Dou</a>*,
        <a href="https://haolirobo.github.io/" target="_blank">Hao Li</a>*,
				<a href="https://tanmay-agarwal.com/" target="_blank">Tanmay Agarwal</a>,
        <a href="http://web.stanford.edu/~bohg/" target="_blank">Jeannette Bohg</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
        <a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei</a>, and
				<a href="http://jiajunwu.com/" target="_blank">Jiajun Wu</a>
				<br>
				<b><font color="black">The ObjectFolder Benchmark: Multisensory Learning with Neural and Real Objects</font></b><br>
				<b><a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a></b>,
				<a href="https://objectfolder.stanford.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://arxiv.org/abs/2306.00956" target="_blank"> <small>[Paper]</small></a>
				<a href="https://www.objectfolder.org/swan_vis/" target="_blank"> <small>[Demo]</small></a>
				<a href="https://yunzhuli.github.io/projects/objectfolderbenchmark/objectfolderbenchmark.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/ActionSense/ActionSense.jpg"/*tpa=https://yunzhuli.github.io/projects/ActionSense/ActionSense.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://www.josephdelpreto.com/" target="_blank">Joseph DelPreto</a>*,
				<a href="https://chaoliu.tech/" target="_blank">Chao Liu</a>*,
				<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
				<a href="https://danielarus.csail.mit.edu/" target="_blank">Daniela Rus</a>
				<br>
				<b><font color="black">ActionSense: A Multimodal Dataset and Recording Framework for Human Activities Using Wearable Sensors in a Kitchen Environment</font></b><br>
				<b><a href="https://neurips.cc/Conferences/2022" target="_blank">NeurIPS 2022 Datasets and Benchmarks</a></b>,
				<a href="https://action-net.csail.mit.edu/" target="_blank"> <small>[Project & Video]</small></a>
				<a href="https://openreview.net/forum?id=olvz0gAdGOX" target="_blank"> <small>[Paper]</small></a>
				<a href="https://yunzhuli.github.io/projects/ActionSense/ActionSense.bib" target="_blank"> <small>[BibTex]</small></a><br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/senstextile/senstextile.jpg"/*tpa=https://yunzhuli.github.io/projects/senstextile/senstextile.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="http://people.csail.mit.edu/kuiwu" target="_blank">Kui Wu</a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/beichen-li" target="_blank">Beichen Li</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning Human-environment Interactions using Conformal Tactile Textiles</font></b><br>
				<b><a href="https://www.nature.com/natelectron/" target="_blank">Nature Electronics</a></b> 4, 193–201 (2021),
				<font color="firebrick"><b>5-year Impact Factor: 33.695</b></font>
				<br>
				<a href="http://senstextile.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41928-021-00558-0" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/YunzhuLi/senstextile" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/senstextile/senstextile.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Featured on the</small>
				<a href="https://www.nature.com/natelectron/volumes/4/issues/3" target="_blank"> <small>cover</small></a>
				<small>of the issue.</small>
				<small>Editorial comments</small>
				<a href="https://www.nature.com/articles/s41928-021-00567-z" target="_blank"> <small>[Link]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://www.nature.com/articles/s41928-021-00560-6" target="_blank"> <small>[Nature Electronics News & Views]</small></a>
				<a href="https://www.csail.mit.edu/news/smart-clothes-can-measure-your-movements" target="_blank"> <small>[MIT CSAIL News]</small></a>
				<a href="https://gizmodo.com/researchers-might-have-finally-cracked-smart-clothing-1846546202" target="_blank"> <small>[Gizmodo]</small></a>
				<a href="https://www.engadget.com/mit-csail-smart-clothes-track-movements-160010512.html" target="_blank"> <small>[Engadget]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/phystouch/phystouch.jpeg"/*tpa=https://yunzhuli.github.io/projects/phystouch/phystouch.jpeg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
					<a href="https://github.com/sjtuzq" target="_blank">Qiang Zhang</a>*,
					<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
					<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
					<a href="https://showone90.wixsite.com/show" target="_blank">Wan Shou</a>,
					<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
					<a href="https://thinklab.sjtu.edu.cn/" target="_blank">Junchi Yan</a>,
					<a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en" target="_blank">Joshua B. Tenenbaum</a>,
					<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>, and
					<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
					<br>
					<b><font color="black">Dynamic Modeling of Hand-Object Interactions via Tactile Sensing</font></b><br>
					<b><a href="https://www.iros2021.org/" target="_blank">IROS 2021</a></b>,
					<a href="http://phystouch.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
					<a href="https://arxiv.org/abs/2109.04378" target="_blank"> <small>[Paper]</small></a>
					<a href="https://www.youtube.com/watch?v=rBN5kNOw5Y8" target="_blank"> <small>[Video]</small></a>
					<a href="https://yunzhuli.github.io/projects/phystouch/phystouch.bib" target="_blank"> <small>[BibTex]</small></a>
					<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/intcarpet/logo.jpg"/*tpa=https://yunzhuli.github.io/projects/intcarpet/logo.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
        			<a href="https://yyueluo.com/" target="_blank">Yiyue Luo</a>,
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
        			<a href="https://www.csail.mit.edu/person/michael-foshey" target="_blank">Michael Foshey</a>,
        			<a href="https://www.csail.mit.edu/person/wan-shou" target="_blank">Wan Shou</a>,
        			<a href="https://pratyushasharma.github.io/" target="_blank">Pratyusha Sharma</a>,
        			<a href="http://www-mtl.mit.edu/wpmu/tpalacios/" target="_blank">Tomas Palacios</a>,
        			<a href="https://groups.csail.mit.edu/vision/torralbalab/" target="_blank">Antonio Torralba</a>, and
        			<a href="https://cdfg.mit.edu/wojciech" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Intelligent Carpet: Inferring 3D Human Pose from Tactile Signals</font></b><br>
				<b><a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a></b>,
				<a href="http://intcarpet.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Intelligent_Carpet_Inferring_3D_Human_Pose_From_Tactile_Signals_CVPR_2021_paper.pdf" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/yiyueluo/IntelligentCarpet" target="_blank"> <small>[Code]</small></a>
				<a href="https://www.youtube.com/watch?v=U6svj37h2U4" target="_blank"> <small>[Video]</small></a>
				<a href="https://yunzhuli.github.io/projects/intcarpet/intcarpet.bib" target="_blank"> <small>[BibTex]</small></a>
				<br>
				<small>Covered by</small>
				<a href="https://news.mit.edu/2021/intelligent-carpet-gives-insight-human-poses-0624" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.fastcompany.com/90648670/this-magic-carpet-can-track-your-workout" target="_blank"> <small>[Fast Company]</small></a>
				<br>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/stag/stag_lowres.jpg"/*tpa=https://yunzhuli.github.io/projects/stag/stag_lowres.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="https://people.csail.mit.edu/subras/" target="_blank">Subramanian Sundaram</a>,
				<a href="https://people.csail.mit.edu/pkellnho/" target="_blank">Petr Kellnhofer</a>,
				<a href="https://people.csail.mit.edu/liyunzhu/"><b>Yunzhu Li</b></a>,
				<a href="https://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>, and
				<a href="https://people.csail.mit.edu/wojciech/" target="_blank">Wojciech Matusik</a>
				<br>
				<b><font color="black">Learning the Signatures of the Human Grasp Using a Scalable Tactile Glove</font></b><br>
				<b><a href="https://www.nature.com/" target="_blank">Nature</a></b> 569, 698–702 (2019),
				<font color="firebrick"><b>5-year Impact Factor: 54.637</b></font>
				<br>
				<a href="http://stag.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="https://www.nature.com/articles/s41586-019-1234-z" target="_blank"> <small>[Paper]</small></a>
				<a href="https://github.com/Erkil1452/touch" target="_blank"> <small>[Code]</small></a>
				<a href="http://stag.csail.mit.edu/files/sundaram2019stag.bib" target="_blank"> <small>[BibTex]</small></a>,

				<small>Collected by</small>
				<a href="projects/stag/stag_mit_museum.jpg"/*tpa=https://yunzhuli.github.io/projects/stag/stag_mit_museum.jpg*/ target="_blank"> <small>MIT Museum</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/sensor-glove-human-grasp-robotics-0529" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.nature.com/articles/d41586-019-01593-w" target="_blank"> <small>[Nature News & Views]</small></a>
				<a href="https://devicematerialscommunity.nature.com/users/257334-subramanian-sundaram/posts/49420-learning-dexterity-from-humans" target="_blank"> <small>[Nature communities]</small></a>
				<a href="https://www.economist.com/science-and-technology/2019/05/30/improving-robots-grasp-requires-a-new-way-to-measure-it-in-humans" target="_blank"> <small>[The Economist]</small></a>
				<a href="https://www.pbs.org/wgbh/nova/article/electronic-glove-pressure-sensors/" target="_blank"> <small>[PBS NOVA]</small></a>
				<a href="https://www.bbc.co.uk/sounds/play/p079yr9y" target="_blank"> <small>[BBC Radio]</small></a>
				<a href="https://www.newscientist.com/article/2204736-smart-glove-works-out-what-youre-holding-from-its-weight-and-shape/" target="_blank"> <small>[NewScientist]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/visgel/visgel.jpg"/*tpa=https://yunzhuli.github.io/projects/visgel/visgel.jpg*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://people.csail.mit.edu/junyanz/" target="_blank">Jun-Yan Zhu</a>,
				<a href="https://groups.csail.mit.edu/locomotion/russt.html" target="_blank">Russ Tedrake</a>, and
				<a href="http://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>
				<br>
				<b><font color="black">Connecting Touch and Vision via Cross-Modal Prediction</font></b><br>
				<b><a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a></b>,
				<a href="http://visgel.csail.mit.edu/" target="_blank"> <small>[Project]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/VisGel" target="_blank"><small>[Code]</small></a>
				<a href="http://visgel.csail.mit.edu/visgel.bib" target="_blank"> <small>[BibTex]</small></a><br>

				<small>Covered by</small>
				<a href="http://news.mit.edu/2019/teaching-ai-to-connect-senses-vision-touch-0617" target="_blank"> <small>[MIT News]</small></a>
				<a href="https://www.bbc.com/news/av/technology-48711479/robot-taught-to-feel-objects-by-sight-and-other-news" target="_blank"> <small>[BBC]</small></a>
				<a href="https://www.cnn.com/2019/06/17/us/mit-robot-vision-touch-trnd/index.html" target="_blank"> <small>[CNN]</small></a>
				<a href="https://www.forbes.com/sites/charlestowersclark/2019/06/17/one-step-closer-to-human-intelligence-mit-csail-combine-sight-and-touch-in-ai/#3496256578b6" target="_blank"> <small>[Forbes]</small></a>
				<a href="https://techcrunch.com/2019/06/17/mit-develops-a-system-to-give-robots-more-human-senses/" target="_blank"> <small>[TechCrunch]</small></a>
				<a href="https://www.fastcompany.com/90365007/a-new-robot-can-now-identify-objects-by-touch" target="_blank"> <small>[Fast Company]</small></a>
				<a href="https://www.engadget.com/2019/06/17/robot-identify-sight-touch/" target="_blank"> <small>[Engadget]</small></a>
				</div>
			</div><hr>


			<br>
			<div id="imi" style="padding-top: 80px; margin-top: -80px;">
				<h5>Imitation Learning</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/infogail/infogail.png"/*tpa=https://yunzhuli.github.io/projects/infogail/infogail.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>,
				<a href="http://tsong.me/" target="_blank">Jiaming Song</a>, and
				<a href="http://cs.stanford.edu/~ermon/" target="_blank">Stefano Ermon</a>
				<br>
				<b><font color="black">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</font></b><br>
				<b><a href="https://nips.cc/Conferences/2017" target="_blank">NIPS 2017</a></b>,
				<a href="https://yunzhuli.github.io/projects/infogail/infogail-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/YunzhuLi/InfoGAIL" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/infogail/infogail.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/infogail/infogail-poster.pdf" target="_blank"><small>[Poster]</small></a>
				<a href="https://www.youtube.com/watch?v=YtNPBAW6h5k" target="_blank"> <small>[Video]</small></a>
				</div>
			</div><hr>


			<br>
			<div id="det" style="padding-top: 80px; margin-top: -80px;">
				<h5>Detection & Segmentation</h5>
			</div><br>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/skin/detection_tracking.png"/*tpa=https://yunzhuli.github.io/projects/skin/detection_tracking.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				<a href="http://cs.stanford.edu/people/esteva/home/index.html" target="_blank">Andre Esteva</a>*,
				<a href="https://stanford.edu/~kuprel/" target="_blank">Brett Kuprel</a>,
				<a href="https://profiles.stanford.edu/roberto-novoa" target="_blank">Rob Novoa</a>,
				<a href="https://profiles.stanford.edu/justin-ko" target="_blank">Justin Ko</a>, and
				<a href="http://robots.stanford.edu/" target="_blank">Sebastian Thrun</a>
				<br>
				<b><font color="black">Skin Cancer Detection and Tracking using Data Synthesis and Deep Learning</font></b><br>
				<a href="https://nips.cc/Conferences/2016" target="_blank"><b>NIPS 2016</b> Workshop on Machine Learning for Health</a><br>
				<a href="http://w3phiai2017.w3phi.com/" target="_blank"><b>AAAI 2017</b> Joint Workshop on Health Intelligence</a><br>
				<a href="https://yunzhuli.github.io/projects/skin/detection_tracking-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://yunzhuli.github.io/projects/skin/skin.bib" target="_blank"> <small>[BibTex]</small></a>
				</div>
			</div><hr>

			<div class="row">
				<div class="col-md-3">
					<img class="img-fluid img-rounded" src="projects/conv3d/face_detection_conv3d.png"/*tpa=https://yunzhuli.github.io/projects/conv3d/face_detection_conv3d.png*/ style="border:1px solid black" alt="">
				</div>
				<div class="col-md-9">
				<a href="index.htm"/*tpa=https://yunzhuli.github.io/*/><b>Yunzhu Li</b></a>*,
				Benyuan Sun*,
				<a href="http://www.stat.ucla.edu/~tfwu/" target="_blank">Tianfu Wu</a>, and
				<a href="http://www.idm.pku.edu.cn/staff/wangyizhou/" target="_blank">Yizhou Wang</a>
				<br>
				<b><font color="black">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</font></b><br>
				<b><a href="http://www.eccv2016.org/" target="_blank">ECCV 2016</a></b>,
				<a href="https://yunzhuli.github.io/projects/conv3d/face_detection_conv3d-paper.pdf" target="_blank"> <small>[PDF]</small></a>
				<a href="https://github.com/tfwu/FaceDetection-ConvNet-3D" target="_blank"> <small>[Code]</small></a>
				<a href="https://yunzhuli.github.io/projects/conv3d/conv3d.bib" target="_blank"> <small>[BibTex]</small></a>
				<a href="https://yunzhuli.github.io/projects/conv3d/face_detection_conv3d-poster.pdf" target="_blank"> <small>[Poster]</small></a>
				</div>
			</div><hr>

		</script>

	</div>
	
	
	<br><br>


	<!-- Awards -->
	<div class="container">
		<h3 id="Honors" style="padding-top: 80px; margin-top: -80px;">Selected Honors</h3>
		<ul>
			<li>Best Systems Paper Award, Conference on Robot Learning (CoRL) 2023 <a href="javascript:if(confirm(%27https://www.corl2023.org/awards  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.corl2023.org/awards%27" tppabs="https://www.corl2023.org/awards" target="_blank"><small>[Link]</small></a></li>
			<li>Finalist, Best Paper/Best Student Paper Awards, Conference on Robot Learning (CoRL) 2023 <a href="javascript:if(confirm(%27https://www.corl2023.org/awards  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.corl2023.org/awards%27" tppabs="https://www.corl2023.org/awards" target="_blank"><small>[Link]</small></a></li>
			<li>First Place Recipient, Ernst A. Guillemin Master's Thesis Award in Artificial Intelligence and Decision Making at MIT, 2021 <a href="javascript:if(confirm(%27https://www.eecs.mit.edu/2021-eecs-awards/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.eecs.mit.edu/2021-eecs-awards/%27" tppabs="https://www.eecs.mit.edu/2021-eecs-awards/" target="_blank"><small>[Link]</small></a></li>
			<li>Adobe Research Fellowship, 2020 <a href="javascript:if(confirm(%27https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/%27" tppabs="https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/" target="_blank"><small>[Link]</small></a></li>
			<li>Finalist, Facebook Fellowship, 2021 <a href="javascript:if(confirm(%27https://research.fb.com/blog/2021/04/announcing-the-recipients-of-the-2021-facebook-fellowship-awards/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://research.fb.com/blog/2021/04/announcing-the-recipients-of-the-2021-facebook-fellowship-awards/%27" tppabs="https://research.fb.com/blog/2021/04/announcing-the-recipients-of-the-2021-facebook-fellowship-awards/" target="_blank"><small>[Link]</small></a></li>
			<li>Finalist, NVIDIA Graduate Fellowship, 2019, 2020, 2021 <a href="javascript:if(confirm(%27https://www.nvidia.com/en-us/research/graduate-fellowships/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.nvidia.com/en-us/research/graduate-fellowships/%27" tppabs="https://www.nvidia.com/en-us/research/graduate-fellowships/" target="_blank"><small>[Link]</small></a></li>
			<li>Outstanding Reviewer, ICCV 2021 (Top 5%) <a href="javascript:if(confirm(%27http://iccv2021.thecvf.com/outstanding-reviewers  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27http://iccv2021.thecvf.com/outstanding-reviewers%27" tppabs="http://iccv2021.thecvf.com/outstanding-reviewers" target="_blank"><small>[Link]</small></a></li>
			<li>Outstanding Undergraduate Thesis Award, EECS Dept., Peking Univ. (10 in 400)</li>
		</ul>
	</div><br><br>


	<!-- Service -->
	<div class="container">
		<h3 id="Service" style="padding-top: 80px; margin-top: -80px;">Professional Service</h3>
		<ul>
			<li>Workshop Organizer</li>
				<ul>
					<li><a href="javascript:if(confirm(%27https://imrss2022.github.io/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://imrss2022.github.io/%27" tppabs="https://imrss2022.github.io/" target="_blank">Implicit Representations for Robotic Manipulation</a> at RSS 2022</li>
					<li><a href="javascript:if(confirm(%27https://www.mair2.com/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.mair2.com/%27" tppabs="https://www.mair2.com/" target="_blank">Multi-Agent Interaction and Relational Reasoning</a> at ICCV 2021</li>
					<li><a href="javascript:if(confirm(%27https://sites.google.com/nvidia.com/do-sim/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://sites.google.com/nvidia.com/do-sim/%27" tppabs="https://sites.google.com/nvidia.com/do-sim/" target="_blank">Deformable Object Simulation in Robotics</a> at RSS 2021</li>
					<li><a href="javascript:if(confirm(%27https://www.visionmeetscognition.org/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.visionmeetscognition.org/%27" tppabs="https://www.visionmeetscognition.org/" target="_blank">Vision Meets Cognition</a> at CVPR 2019</li>
				</ul>
			<li>Tutorial Organizer</li>
				<ul>
					<li><a href="javascript:if(confirm(%27https://xiaolonw.github.io/graphnnv3/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://xiaolonw.github.io/graphnnv3/%27" tppabs="https://xiaolonw.github.io/graphnnv3/" target="_blank">Learning Representations via Graph-structured Networks</a> at CVPR 2021</li>
					<li><a href="javascript:if(confirm(%27https://bryanyzhu.github.io/mm-iccv2021/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://bryanyzhu.github.io/mm-iccv2021/%27" tppabs="https://bryanyzhu.github.io/mm-iccv2021/" target="_blank">Multi-Modality Learning from Videos and Beyond</a> at ICCV 2021</li>
				</ul>
			<li>Area Chair: CoRL, CVPR</li>
			<li>Senior Program Committee Member: AAAI</li>
			<li>Conference Reviewer: ICLR, ICML, NeurIPS, ICCV, CVPR, ECCV, ICRA, IROS, RSS, CoRL, Humanoids</li>
			<li>Program Committee Member: AAAI, UAI</li>
			<li>Journal Reviewer:
				<a href="javascript:if(confirm(%27https://www.science.org/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.science.org/%27" tppabs="https://www.science.org/" target="_blank">Science</a>,
				<a href="javascript:if(confirm(%27https://www.computer.org/csdl/journal/tp  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.computer.org/csdl/journal/tp%27" tppabs="https://www.computer.org/csdl/journal/tp" target="_blank">TPAMI</a>,
				<a href="javascript:if(confirm(%27https://www.springer.com/journal/11263  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.springer.com/journal/11263%27" tppabs="https://www.springer.com/journal/11263" target="_blank">IJCV</a>,
				<a href="javascript:if(confirm(%27https://www.ieee-ras.org/publications/t-ro  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.ieee-ras.org/publications/t-ro%27" tppabs="https://www.ieee-ras.org/publications/t-ro" target="_blank">IEEE T-RO</a>,
				<a href="javascript:if(confirm(%27https://www.ieee-ras.org/publications/ra-l  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://www.ieee-ras.org/publications/ra-l%27" tppabs="https://www.ieee-ras.org/publications/ra-l" target="_blank">IEEE RA-L</a>,
				<a href="javascript:if(confirm(%27https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42%27" tppabs="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42" target="_blank">IEEE-TMI</a>,
				<a href="javascript:if(confirm(%27https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6979  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6979%27" tppabs="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6979" target="_blank">IEEE T-ITS</a>,
				<a href="javascript:if(confirm(%27https://ieeeaccess.ieee.org/  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://ieeeaccess.ieee.org/%27" tppabs="https://ieeeaccess.ieee.org/" target="_blank">IEEE Access</a>,
				<a href="javascript:if(confirm(%27https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4543165  \n\nThis file was not retrieved by Teleport Ultra, because it is addressed on a domain or path outside the boundaries set for its Starting Address.  \n\nDo you want to open it from the server?%27))window.location=%27https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4543165%27" tppabs="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4543165" target="_blank">IEEE-ToH</a>
			</li>
		</ul>
	</div>

	<div class="container">
		<hr>
		<center>
			<footer>
				<p>&copy; University of Illinois Urbana-Champaign 2023</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script>showPubs(1);</script>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
	<script src="../code.jquery.com/jquery-3.2.1.slim.min.js" tppabs="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="../cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" tppabs="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
	<script src="../maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" tppabs="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>
</body>

</html>
